{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQfRDGYIG5e7r7Q94sCbp/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sino-taku/PyTorch_Reference-/blob/main/Binary_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n45onocLqsI_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2値分布\n",
        "\n",
        "モデルの構造を説明します。本章は「アイリス・データセット」と呼ばれる公開データセットを利用します。\n",
        "\n",
        "アヤメのデータです。\n",
        "\n",
        "今回重要なのは「精度」という指標値を導入しておりよりモデルの性能を判断しやすくしています。\n",
        "「精度」の概念と関連した話として「訓練データと検証データの分割」という考え方が出てきます。\n"
      ],
      "metadata": {
        "id": "QCsgRNmiqwfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#精度（Accuracy)\n",
        "\n",
        "これは尤度の考え方です。尤度のノートを振りかえってください。"
      ],
      "metadata": {
        "id": "MP-6-qXBumFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#シグモイド関数\n",
        "\n",
        "まず理解しやすいようにグラフを表示します。\n",
        "\n",
        "この特徴は0と1の間をとる点でかつ対象になっています。\n",
        "\n",
        "これを利用することで微小な変化を1か0であらわすことが容易になります。"
      ],
      "metadata": {
        "id": "_15qwtSovkPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要ライブラリの導入\n",
        "\n",
        "!pip install japanize_matplotlib | tail -n 1\n",
        "!pip install torchviz | tail -n 1\n",
        "!pip install torchinfo | tail -n 1\n",
        "\n",
        "# 必要ライブラリのインポート\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "from IPython.display import display\n",
        "\n",
        "# torch関連ライブラリのインポート\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "from torchviz import make_dot\n",
        "\n",
        "# デフォルトフォントサイズ変更\n",
        "plt.rcParams['font.size'] = 14\n",
        "\n",
        "# デフォルトグラフサイズ変更\n",
        "plt.rcParams['figure.figsize'] = (6,6)\n",
        "\n",
        "# デフォルトで方眼表示ON\n",
        "plt.rcParams['axes.grid'] = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbHe0j4CwtZT",
        "outputId": "4b460cfe-cb59-4772-b455-aa18bef7c5b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed japanize-matplotlib-1.1.3\n",
            "Successfully installed torchviz-0.0.2\n",
            "Successfully installed torchinfo-1.6.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NumPy配列でxデータを定義\n",
        "x_np = np.arange(-4, 4.1, 0.25)\n",
        "\n",
        "# データをTensor形式に変換\n",
        "x = torch.tensor(x_np).float()\n",
        "\n",
        "# yの値を計算\n",
        "y = torch.sigmoid(x)\n",
        "\n",
        "# グラフ描画\n",
        "plt.title('シグモイド関数のグラフ')\n",
        "plt.plot(x.data, y.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "npUJf0rGw9OC",
        "outputId": "e210a69b-c811-4351-d832-4fd1b4ca439d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8facaf9e10>]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#交差エントロピー関数\n",
        "\n",
        "シグモイド関数の出力として得られる確率値は厳密には「入力データに対して分類結果が1になる確率」であることです。2値分布の場合正解値は0か1なので「分類結果が1になる確率」がfaなら「分類結果が0になる確率」は1-faとなる\n",
        "\n",
        "ここで損失関数に最尤推定という考えを導入します。\n",
        "\n",
        "ノートを振り返ってください"
      ],
      "metadata": {
        "id": "oucvpXdqxNWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#データの用意"
      ],
      "metadata": {
        "id": "AFUA3VEOyzXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習用データ準備\n",
        "\n",
        "# ライブラリのインポート\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# データ読み込み\n",
        "iris = load_iris()\n",
        "\n",
        "# 入力データと正解データ取得\n",
        "x_org, y_org = iris.data, iris.target\n",
        "\n",
        "# 結果確認\n",
        "print('元データ', x_org.shape, y_org.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGlE8Z2zy5W0",
        "outputId": "3c88d9be-704a-4ed8-8d2e-f7b3e1e7cff4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "元データ (150, 4) (150,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q　データの絞り込み\n",
        "\n",
        "データを絞って2種類のみになるようにします。"
      ],
      "metadata": {
        "id": "g0R6yyK-y_kZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データ絞り込み\n",
        "#   クラス0, 1のみ\n",
        "#   項目sepal_lengthとsepal_widthのみ\n",
        "\n",
        "x_data = iris.data[:100,:2]\n",
        "y_data = iris.target[:100]\n",
        "\n",
        "# 結果確認\n",
        "print('対象データ', x_data.shape, y_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE12r2FazWY2",
        "outputId": "7ee2def6-5c04-4195-d084-3852c892e230"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "対象データ (100, 2) (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q　訓練データと検証データへの分割\n",
        "\n",
        "やり方は前回に記載"
      ],
      "metadata": {
        "id": "PKvw5M_2zteE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 　元データのサイズ\n",
        "print(x_data.shape, y_data.shape)\n",
        "\n",
        "# 訓練データ、検証データに分割 (シャフルも同時に実施)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_data, y_data, train_size=70, test_size=30, \n",
        "    random_state=123)\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-k4vSERz_cZ",
        "outputId": "c024d930-22c8-42e1-a4fc-6ec888555272"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 2) (100,)\n",
            "(70, 2) (30, 2) (70,) (30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#散布図表示\n",
        "\n"
      ],
      "metadata": {
        "id": "7Ld4Gwxm0Y7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 散布図の表示\n",
        "\n",
        "x_t0 = x_train[y_train == 0]\n",
        "x_t1 = x_train[y_train == 1]\n",
        "plt.scatter(x_t0[:,0], x_t0[:,1], marker='x', c='b', label='0 (setosa)')\n",
        "plt.scatter(x_t1[:,0], x_t1[:,1], marker='o', c='k', label='1 (versicolor)')\n",
        "plt.xlabel('sepal_length')\n",
        "plt.ylabel('sepal_width')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "H9A6d_kx0tgz",
        "outputId": "8d9b5478-c3de-4066-f3dc-dcb1d0cbfc67"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#モデルの定義\n",
        "\n",
        "重回帰と同じモデル"
      ],
      "metadata": {
        "id": "B7jPFhtQ1IA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 入力次元数　(今の場合2)\n",
        "n_input= x_train.shape[1]\n",
        "\n",
        "# 出力次元数\n",
        "n_output = 1\n",
        "\n",
        "# 結果確認\n",
        "print(f'n_input: {n_input}  n_output:{n_output}')\n",
        "\n",
        "# モデルの定義\n",
        "# 2入力1出力のロジスティック回帰モデル\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_input, n_output):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(n_input, n_output)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "                \n",
        "        # 初期値を全部1にする\n",
        "        # 「ディープラーニングの数学」と条件を合わせる目的        \n",
        "        self.l1.weight.data.fill_(1.0)\n",
        "        self.l1.bias.data.fill_(1.0)        \n",
        "        \n",
        "    # 予測関数の定義\n",
        "    def forward(self, x):\n",
        "        # 最初に入力値を線形関数にかけたを計算する\n",
        "        x1 = self.l1(x)\n",
        "        # 計算結果にシグモイド関数をかける\n",
        "        x2 = self.sigmoid(x1)\n",
        "        return x2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "molXM5281Tib",
        "outputId": "ce9c8609-7b77-4351-e0e5-871f82588549"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_input: 2  n_output:1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "シグモイド関数を導入していることを確認しよう\n",
        "\n",
        "次にサマリー表示していきましょう"
      ],
      "metadata": {
        "id": "uuqT-EY718GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# インスタンスの生成\n",
        "\n",
        "net = Net(n_input, n_output)\n",
        "\n",
        "# モデル内のパラメータの確認\n",
        "# l1.weightとl1.biasがあることがわかる\n",
        "\n",
        "for parameter in net.named_parameters():\n",
        "    print(parameter)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtP7Csgo2wlY",
        "outputId": "e9356809-016a-4732-ce97-80700ffe086e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('l1.weight', Parameter containing:\n",
            "tensor([[1., 1.]], requires_grad=True))\n",
            "('l1.bias', Parameter containing:\n",
            "tensor([1.], requires_grad=True))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの概要表示\n",
        "\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxx4gphR3puW",
        "outputId": "10318980-5ec1-4f28-e7be-d1b77af44c10"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (l1): Linear(in_features=2, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルのサマリー表示\n",
        "\n",
        "summary(net, (2,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg60rMYq3wur",
        "outputId": "af433942-1d00-45f5-bc2b-36108741f70f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      --                        --\n",
              "├─Linear: 1-1                            [1]                       3\n",
              "├─Sigmoid: 1-2                           [1]                       --\n",
              "==========================================================================================\n",
              "Total params: 3\n",
              "Trainable params: 3\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q　最適化アルゴリズムと損失関数\n",
        "\n",
        "損失関数として交差エントロピー関数を使用していることを確認しましょう。"
      ],
      "metadata": {
        "id": "EHivfRRD5ffe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 損失関数： 交差エントロピー関数\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# 学習率\n",
        "lr = 0.01\n",
        "\n",
        "# 最適化関数: 勾配降下法\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "bNniM6ao6uUb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#勾配降下法\n",
        "\n",
        "テンソル化と計算グラフの図示をします。"
      ],
      "metadata": {
        "id": "3Cqx6X3p6LpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 入力データ x_train と正解データ y_train のテンソル化\n",
        "\n",
        "inputs = torch.tensor(x_train).float()\n",
        "labels = torch.tensor(y_train).float()\n",
        "\n",
        "# 正解データはN行1列の行列に変換する\n",
        "labels1 = labels.view((-1,1))\n",
        "\n",
        "# 検証データのテンソル化\n",
        "inputs_test = torch.tensor(x_test).float()\n",
        "labels_test = torch.tensor(y_test).float()\n",
        "\n",
        "# 検証用の正解データもN行1列の行列に変換する\n",
        "labels1_test = labels_test.view((-1,1))\n",
        "\n",
        "# 予測計算\n",
        "outputs = net(inputs)\n",
        "\n",
        "# 損失計算\n",
        "loss = criterion(outputs, labels1)\n",
        "\n",
        "# 損失の計算グラフ可視化\n",
        "g = make_dot(loss, params=dict(net.named_parameters()))\n",
        "display(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "NSG_Fp9O6iTR",
        "outputId": "0776feaf-7ac7-487e-fa26-8a26b483b529"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f8f9dab7f90>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"216pt\" height=\"391pt\"\n viewBox=\"0.00 0.00 216.00 391.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 387)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-387 212,-387 212,4 -4,4\"/>\n<!-- 140254802282448 -->\n<g id=\"node1\" class=\"node\">\n<title>140254802282448</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"130.5,-31 76.5,-31 76.5,0 130.5,0 130.5,-31\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140254802311632 -->\n<g id=\"node2\" class=\"node\">\n<title>140254802311632</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"193,-86 14,-86 14,-67 193,-67 193,-86\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">BinaryCrossEntropyBackward0</text>\n</g>\n<!-- 140254802311632&#45;&gt;140254802282448 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140254802311632&#45;&gt;140254802282448</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M103.5,-66.9688C103.5,-60.1289 103.5,-50.5621 103.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"107.0001,-41.3678 103.5,-31.3678 100.0001,-41.3678 107.0001,-41.3678\"/>\n</g>\n<!-- 140254802312848 -->\n<g id=\"node3\" class=\"node\">\n<title>140254802312848</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"160,-141 47,-141 47,-122 160,-122 160,-141\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SigmoidBackward0</text>\n</g>\n<!-- 140254802312848&#45;&gt;140254802311632 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140254802312848&#45;&gt;140254802311632</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M103.5,-121.9197C103.5,-114.9083 103.5,-105.1442 103.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"107.0001,-96.3408 103.5,-86.3408 100.0001,-96.3409 107.0001,-96.3408\"/>\n</g>\n<!-- 140254802311184 -->\n<g id=\"node4\" class=\"node\">\n<title>140254802311184</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"154,-196 53,-196 53,-177 154,-177 154,-196\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward0</text>\n</g>\n<!-- 140254802311184&#45;&gt;140254802312848 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140254802311184&#45;&gt;140254802312848</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M103.5,-176.9197C103.5,-169.9083 103.5,-160.1442 103.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"107.0001,-151.3408 103.5,-141.3408 100.0001,-151.3409 107.0001,-151.3408\"/>\n</g>\n<!-- 140254802310608 -->\n<g id=\"node5\" class=\"node\">\n<title>140254802310608</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-251 0,-251 0,-232 101,-232 101,-251\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140254802310608&#45;&gt;140254802311184 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140254802310608&#45;&gt;140254802311184</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M59.7319,-231.9197C67.2391,-224.1293 78.021,-212.9405 87.0049,-203.6176\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"89.5983,-205.9703 94.017,-196.3408 84.5578,-201.113 89.5983,-205.9703\"/>\n</g>\n<!-- 140255053727792 -->\n<g id=\"node6\" class=\"node\">\n<title>140255053727792</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"80,-317 21,-317 21,-287 80,-287 80,-317\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">l1.bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140255053727792&#45;&gt;140254802310608 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140255053727792&#45;&gt;140254802310608</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-286.7333C50.5,-279.0322 50.5,-269.5977 50.5,-261.3414\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-261.0864 50.5,-251.0864 47.0001,-261.0864 54.0001,-261.0864\"/>\n</g>\n<!-- 140254802312016 -->\n<g id=\"node7\" class=\"node\">\n<title>140254802312016</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"196,-251 119,-251 119,-232 196,-232 196,-251\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward0</text>\n</g>\n<!-- 140254802312016&#45;&gt;140254802311184 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140254802312016&#45;&gt;140254802311184</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M148.0939,-231.9197C140.4451,-224.1293 129.4597,-212.9405 120.3064,-203.6176\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.6653,-201.0244 113.1619,-196.3408 117.6704,-205.9286 122.6653,-201.0244\"/>\n</g>\n<!-- 140254802310544 -->\n<g id=\"node8\" class=\"node\">\n<title>140254802310544</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"208,-311.5 107,-311.5 107,-292.5 208,-292.5 208,-311.5\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140254802310544&#45;&gt;140254802312016 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140254802310544&#45;&gt;140254802312016</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M157.5,-292.2796C157.5,-284.0376 157.5,-271.9457 157.5,-261.629\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"161.0001,-261.3972 157.5,-251.3972 154.0001,-261.3973 161.0001,-261.3972\"/>\n</g>\n<!-- 140254802925712 -->\n<g id=\"node9\" class=\"node\">\n<title>140254802925712</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"193,-383 122,-383 122,-353 193,-353 193,-383\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">l1.weight</text>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 2)</text>\n</g>\n<!-- 140254802925712&#45;&gt;140254802310544 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140254802925712&#45;&gt;140254802310544</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M157.5,-352.6924C157.5,-343.5067 157.5,-331.7245 157.5,-321.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"161.0001,-321.703 157.5,-311.7031 154.0001,-321.7031 161.0001,-321.703\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "線形関数の計算した後で「シグモイド関数」と「交差エントロピー」の計算を経て損失を求めています。"
      ],
      "metadata": {
        "id": "atCckWU69CKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KDB61TUU9zys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 初期化処理\n",
        "\n",
        "history変数が5列に増えた理由を説明します。\n",
        "データを訓練と検証データに分けたので損失もぬ辰になりました。そして精度が含まれたので\n",
        "\n",
        "繰り返しの数、訓練・検証の損失・精度の5つになりました。"
      ],
      "metadata": {
        "id": "r_bmMbdM90hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習率\n",
        "lr = 0.01\n",
        "\n",
        "# 初期化\n",
        "net = Net(n_input, n_output)\n",
        "\n",
        "# 損失関数： 交差エントロピー関数\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# 最適化関数: 勾配降下法\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# 繰り返し回数\n",
        "num_epochs = 10000\n",
        "\n",
        "# 記録用リストの初期化\n",
        "history = np.zeros((0,5))"
      ],
      "metadata": {
        "id": "Brawnl2t_VF5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q　メインループ\n",
        "\n",
        "訓練と予測の違いは「勾配計算」と「パラメータ計算」がなくなった点です。"
      ],
      "metadata": {
        "id": "Px85ejRaMME0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 繰り返し計算メインループ\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # 訓練フェーズ\n",
        "    \n",
        "    #勾配値初期化\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 予測計算\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # 損失計算\n",
        "    loss = criterion(outputs, labels1)\n",
        "\n",
        "    # 勾配計算\n",
        "    loss.backward()\n",
        "    \n",
        "    # パラメータ修正\n",
        "    optimizer.step()\n",
        "\n",
        "    # 損失の保存(スカラー値の取得)\n",
        "    train_loss = loss.item()\n",
        "\n",
        "    # 予測ラベル(1 or 0)計算\n",
        "    predicted = torch.where(outputs < 0.5, 0, 1)\n",
        "    \n",
        "    # 精度計算\n",
        "    train_acc = (predicted == labels1).sum() / len(y_train)\n",
        "\n",
        "    # 予測フェーズ\n",
        "\n",
        "    # 予測計算\n",
        "    outputs_test = net(inputs_test)\n",
        "\n",
        "    # 損失計算\n",
        "    loss_test = criterion(outputs_test, labels1_test)\n",
        "\n",
        "    # 損失の保存（スカラー値の取得）\n",
        "    val_loss =  loss_test.item()\n",
        "        \n",
        "    # 予測ラベル(1 or 0)計算\n",
        "    predicted_test = torch.where(outputs_test < 0.5, 0, 1)\n",
        "\n",
        "    # 精度計算\n",
        "    val_acc = (predicted_test == labels1_test).sum() / len(y_test)\n",
        "    \n",
        "    if ( epoch % 10 == 0):\n",
        "        print (f'Epoch [{epoch}/{num_epochs}], loss: {train_loss:.5f} acc: {train_acc:.5f} val_loss: {val_loss:.5f}, val_acc: {val_acc:.5f}')\n",
        "        item = np.array([epoch, train_loss, train_acc, val_loss, val_acc])\n",
        "        history = np.vstack((history, item))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkb1bawVMkyN",
        "outputId": "9ed74e38-4ead-4cb3-c0a4-4dceb6be5337"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/10000], loss: 4.77289 acc: 0.50000 val_loss: 4.49384, val_acc: 0.50000\n",
            "Epoch [10/10000], loss: 3.80546 acc: 0.50000 val_loss: 3.56537, val_acc: 0.50000\n",
            "Epoch [20/10000], loss: 2.84329 acc: 0.50000 val_loss: 2.64328, val_acc: 0.50000\n",
            "Epoch [30/10000], loss: 1.91613 acc: 0.50000 val_loss: 1.76244, val_acc: 0.50000\n",
            "Epoch [40/10000], loss: 1.17137 acc: 0.50000 val_loss: 1.08537, val_acc: 0.50000\n",
            "Epoch [50/10000], loss: 0.84140 acc: 0.50000 val_loss: 0.81872, val_acc: 0.50000\n",
            "Epoch [60/10000], loss: 0.77087 acc: 0.50000 val_loss: 0.77093, val_acc: 0.50000\n",
            "Epoch [70/10000], loss: 0.75450 acc: 0.34286 val_loss: 0.76105, val_acc: 0.33333\n",
            "Epoch [80/10000], loss: 0.74542 acc: 0.25714 val_loss: 0.75447, val_acc: 0.20000\n",
            "Epoch [90/10000], loss: 0.73734 acc: 0.24286 val_loss: 0.74778, val_acc: 0.16667\n",
            "Epoch [100/10000], loss: 0.72949 acc: 0.24286 val_loss: 0.74098, val_acc: 0.13333\n",
            "Epoch [110/10000], loss: 0.72180 acc: 0.27143 val_loss: 0.73419, val_acc: 0.16667\n",
            "Epoch [120/10000], loss: 0.71423 acc: 0.31429 val_loss: 0.72749, val_acc: 0.20000\n",
            "Epoch [130/10000], loss: 0.70680 acc: 0.41429 val_loss: 0.72087, val_acc: 0.20000\n",
            "Epoch [140/10000], loss: 0.69949 acc: 0.47143 val_loss: 0.71437, val_acc: 0.26667\n",
            "Epoch [150/10000], loss: 0.69230 acc: 0.52857 val_loss: 0.70797, val_acc: 0.30000\n",
            "Epoch [160/10000], loss: 0.68524 acc: 0.60000 val_loss: 0.70167, val_acc: 0.36667\n",
            "Epoch [170/10000], loss: 0.67829 acc: 0.62857 val_loss: 0.69548, val_acc: 0.43333\n",
            "Epoch [180/10000], loss: 0.67147 acc: 0.68571 val_loss: 0.68938, val_acc: 0.50000\n",
            "Epoch [190/10000], loss: 0.66476 acc: 0.75714 val_loss: 0.68339, val_acc: 0.56667\n",
            "Epoch [200/10000], loss: 0.65816 acc: 0.81429 val_loss: 0.67749, val_acc: 0.70000\n",
            "Epoch [210/10000], loss: 0.65168 acc: 0.84286 val_loss: 0.67169, val_acc: 0.70000\n",
            "Epoch [220/10000], loss: 0.64531 acc: 0.85714 val_loss: 0.66599, val_acc: 0.73333\n",
            "Epoch [230/10000], loss: 0.63904 acc: 0.85714 val_loss: 0.66037, val_acc: 0.76667\n",
            "Epoch [240/10000], loss: 0.63288 acc: 0.88571 val_loss: 0.65485, val_acc: 0.80000\n",
            "Epoch [250/10000], loss: 0.62682 acc: 0.88571 val_loss: 0.64942, val_acc: 0.83333\n",
            "Epoch [260/10000], loss: 0.62087 acc: 0.90000 val_loss: 0.64408, val_acc: 0.83333\n",
            "Epoch [270/10000], loss: 0.61501 acc: 0.91429 val_loss: 0.63882, val_acc: 0.83333\n",
            "Epoch [280/10000], loss: 0.60925 acc: 0.92857 val_loss: 0.63364, val_acc: 0.86667\n",
            "Epoch [290/10000], loss: 0.60359 acc: 0.94286 val_loss: 0.62855, val_acc: 0.90000\n",
            "Epoch [300/10000], loss: 0.59803 acc: 0.94286 val_loss: 0.62354, val_acc: 0.90000\n",
            "Epoch [310/10000], loss: 0.59255 acc: 0.94286 val_loss: 0.61861, val_acc: 0.90000\n",
            "Epoch [320/10000], loss: 0.58717 acc: 0.94286 val_loss: 0.61376, val_acc: 0.93333\n",
            "Epoch [330/10000], loss: 0.58187 acc: 0.94286 val_loss: 0.60899, val_acc: 0.93333\n",
            "Epoch [340/10000], loss: 0.57667 acc: 0.97143 val_loss: 0.60429, val_acc: 0.93333\n",
            "Epoch [350/10000], loss: 0.57154 acc: 0.97143 val_loss: 0.59967, val_acc: 0.93333\n",
            "Epoch [360/10000], loss: 0.56650 acc: 0.97143 val_loss: 0.59512, val_acc: 0.93333\n",
            "Epoch [370/10000], loss: 0.56155 acc: 0.98571 val_loss: 0.59064, val_acc: 0.93333\n",
            "Epoch [380/10000], loss: 0.55667 acc: 0.98571 val_loss: 0.58623, val_acc: 0.93333\n",
            "Epoch [390/10000], loss: 0.55188 acc: 0.98571 val_loss: 0.58189, val_acc: 0.93333\n",
            "Epoch [400/10000], loss: 0.54716 acc: 0.98571 val_loss: 0.57762, val_acc: 0.93333\n",
            "Epoch [410/10000], loss: 0.54251 acc: 0.98571 val_loss: 0.57341, val_acc: 0.93333\n",
            "Epoch [420/10000], loss: 0.53795 acc: 0.98571 val_loss: 0.56927, val_acc: 0.93333\n",
            "Epoch [430/10000], loss: 0.53345 acc: 1.00000 val_loss: 0.56519, val_acc: 0.93333\n",
            "Epoch [440/10000], loss: 0.52903 acc: 1.00000 val_loss: 0.56117, val_acc: 0.93333\n",
            "Epoch [450/10000], loss: 0.52467 acc: 1.00000 val_loss: 0.55722, val_acc: 0.93333\n",
            "Epoch [460/10000], loss: 0.52038 acc: 1.00000 val_loss: 0.55333, val_acc: 0.93333\n",
            "Epoch [470/10000], loss: 0.51617 acc: 1.00000 val_loss: 0.54949, val_acc: 0.93333\n",
            "Epoch [480/10000], loss: 0.51201 acc: 1.00000 val_loss: 0.54571, val_acc: 0.93333\n",
            "Epoch [490/10000], loss: 0.50793 acc: 1.00000 val_loss: 0.54199, val_acc: 0.93333\n",
            "Epoch [500/10000], loss: 0.50390 acc: 1.00000 val_loss: 0.53833, val_acc: 0.93333\n",
            "Epoch [510/10000], loss: 0.49994 acc: 1.00000 val_loss: 0.53472, val_acc: 0.93333\n",
            "Epoch [520/10000], loss: 0.49604 acc: 1.00000 val_loss: 0.53116, val_acc: 0.93333\n",
            "Epoch [530/10000], loss: 0.49219 acc: 1.00000 val_loss: 0.52766, val_acc: 0.93333\n",
            "Epoch [540/10000], loss: 0.48841 acc: 1.00000 val_loss: 0.52421, val_acc: 0.93333\n",
            "Epoch [550/10000], loss: 0.48468 acc: 1.00000 val_loss: 0.52080, val_acc: 0.93333\n",
            "Epoch [560/10000], loss: 0.48101 acc: 1.00000 val_loss: 0.51745, val_acc: 0.93333\n",
            "Epoch [570/10000], loss: 0.47740 acc: 1.00000 val_loss: 0.51415, val_acc: 0.93333\n",
            "Epoch [580/10000], loss: 0.47384 acc: 1.00000 val_loss: 0.51089, val_acc: 0.93333\n",
            "Epoch [590/10000], loss: 0.47033 acc: 1.00000 val_loss: 0.50769, val_acc: 0.93333\n",
            "Epoch [600/10000], loss: 0.46687 acc: 1.00000 val_loss: 0.50452, val_acc: 0.93333\n",
            "Epoch [610/10000], loss: 0.46347 acc: 1.00000 val_loss: 0.50141, val_acc: 0.93333\n",
            "Epoch [620/10000], loss: 0.46011 acc: 1.00000 val_loss: 0.49833, val_acc: 0.93333\n",
            "Epoch [630/10000], loss: 0.45680 acc: 1.00000 val_loss: 0.49530, val_acc: 0.93333\n",
            "Epoch [640/10000], loss: 0.45355 acc: 1.00000 val_loss: 0.49232, val_acc: 0.93333\n",
            "Epoch [650/10000], loss: 0.45033 acc: 1.00000 val_loss: 0.48937, val_acc: 0.93333\n",
            "Epoch [660/10000], loss: 0.44717 acc: 1.00000 val_loss: 0.48647, val_acc: 0.93333\n",
            "Epoch [670/10000], loss: 0.44405 acc: 1.00000 val_loss: 0.48360, val_acc: 0.93333\n",
            "Epoch [680/10000], loss: 0.44097 acc: 1.00000 val_loss: 0.48078, val_acc: 0.93333\n",
            "Epoch [690/10000], loss: 0.43794 acc: 1.00000 val_loss: 0.47800, val_acc: 0.93333\n",
            "Epoch [700/10000], loss: 0.43495 acc: 1.00000 val_loss: 0.47525, val_acc: 0.93333\n",
            "Epoch [710/10000], loss: 0.43200 acc: 1.00000 val_loss: 0.47254, val_acc: 0.93333\n",
            "Epoch [720/10000], loss: 0.42909 acc: 1.00000 val_loss: 0.46987, val_acc: 0.93333\n",
            "Epoch [730/10000], loss: 0.42623 acc: 1.00000 val_loss: 0.46723, val_acc: 0.93333\n",
            "Epoch [740/10000], loss: 0.42340 acc: 1.00000 val_loss: 0.46463, val_acc: 0.93333\n",
            "Epoch [750/10000], loss: 0.42061 acc: 1.00000 val_loss: 0.46206, val_acc: 0.93333\n",
            "Epoch [760/10000], loss: 0.41786 acc: 1.00000 val_loss: 0.45953, val_acc: 0.93333\n",
            "Epoch [770/10000], loss: 0.41515 acc: 1.00000 val_loss: 0.45703, val_acc: 0.93333\n",
            "Epoch [780/10000], loss: 0.41247 acc: 1.00000 val_loss: 0.45457, val_acc: 0.93333\n",
            "Epoch [790/10000], loss: 0.40983 acc: 1.00000 val_loss: 0.45213, val_acc: 0.93333\n",
            "Epoch [800/10000], loss: 0.40722 acc: 1.00000 val_loss: 0.44973, val_acc: 0.93333\n",
            "Epoch [810/10000], loss: 0.40465 acc: 1.00000 val_loss: 0.44736, val_acc: 0.93333\n",
            "Epoch [820/10000], loss: 0.40211 acc: 1.00000 val_loss: 0.44502, val_acc: 0.93333\n",
            "Epoch [830/10000], loss: 0.39961 acc: 1.00000 val_loss: 0.44271, val_acc: 0.93333\n",
            "Epoch [840/10000], loss: 0.39714 acc: 1.00000 val_loss: 0.44043, val_acc: 0.93333\n",
            "Epoch [850/10000], loss: 0.39470 acc: 1.00000 val_loss: 0.43818, val_acc: 0.93333\n",
            "Epoch [860/10000], loss: 0.39229 acc: 1.00000 val_loss: 0.43596, val_acc: 0.93333\n",
            "Epoch [870/10000], loss: 0.38992 acc: 1.00000 val_loss: 0.43377, val_acc: 0.93333\n",
            "Epoch [880/10000], loss: 0.38757 acc: 1.00000 val_loss: 0.43160, val_acc: 0.93333\n",
            "Epoch [890/10000], loss: 0.38525 acc: 1.00000 val_loss: 0.42946, val_acc: 0.96667\n",
            "Epoch [900/10000], loss: 0.38297 acc: 1.00000 val_loss: 0.42735, val_acc: 0.96667\n",
            "Epoch [910/10000], loss: 0.38071 acc: 1.00000 val_loss: 0.42526, val_acc: 0.96667\n",
            "Epoch [920/10000], loss: 0.37848 acc: 1.00000 val_loss: 0.42320, val_acc: 0.96667\n",
            "Epoch [930/10000], loss: 0.37628 acc: 1.00000 val_loss: 0.42116, val_acc: 0.96667\n",
            "Epoch [940/10000], loss: 0.37410 acc: 1.00000 val_loss: 0.41915, val_acc: 0.96667\n",
            "Epoch [950/10000], loss: 0.37196 acc: 1.00000 val_loss: 0.41717, val_acc: 0.96667\n",
            "Epoch [960/10000], loss: 0.36983 acc: 1.00000 val_loss: 0.41520, val_acc: 0.96667\n",
            "Epoch [970/10000], loss: 0.36774 acc: 1.00000 val_loss: 0.41327, val_acc: 0.96667\n",
            "Epoch [980/10000], loss: 0.36567 acc: 1.00000 val_loss: 0.41135, val_acc: 0.96667\n",
            "Epoch [990/10000], loss: 0.36362 acc: 1.00000 val_loss: 0.40946, val_acc: 0.96667\n",
            "Epoch [1000/10000], loss: 0.36160 acc: 1.00000 val_loss: 0.40759, val_acc: 0.96667\n",
            "Epoch [1010/10000], loss: 0.35961 acc: 1.00000 val_loss: 0.40574, val_acc: 0.96667\n",
            "Epoch [1020/10000], loss: 0.35763 acc: 1.00000 val_loss: 0.40391, val_acc: 0.96667\n",
            "Epoch [1030/10000], loss: 0.35568 acc: 1.00000 val_loss: 0.40211, val_acc: 0.96667\n",
            "Epoch [1040/10000], loss: 0.35376 acc: 1.00000 val_loss: 0.40032, val_acc: 0.96667\n",
            "Epoch [1050/10000], loss: 0.35186 acc: 1.00000 val_loss: 0.39856, val_acc: 0.96667\n",
            "Epoch [1060/10000], loss: 0.34997 acc: 1.00000 val_loss: 0.39682, val_acc: 0.96667\n",
            "Epoch [1070/10000], loss: 0.34811 acc: 1.00000 val_loss: 0.39509, val_acc: 0.96667\n",
            "Epoch [1080/10000], loss: 0.34628 acc: 1.00000 val_loss: 0.39339, val_acc: 0.96667\n",
            "Epoch [1090/10000], loss: 0.34446 acc: 1.00000 val_loss: 0.39171, val_acc: 0.96667\n",
            "Epoch [1100/10000], loss: 0.34266 acc: 1.00000 val_loss: 0.39004, val_acc: 0.96667\n",
            "Epoch [1110/10000], loss: 0.34089 acc: 1.00000 val_loss: 0.38839, val_acc: 0.96667\n",
            "Epoch [1120/10000], loss: 0.33913 acc: 1.00000 val_loss: 0.38677, val_acc: 0.96667\n",
            "Epoch [1130/10000], loss: 0.33739 acc: 1.00000 val_loss: 0.38516, val_acc: 0.96667\n",
            "Epoch [1140/10000], loss: 0.33568 acc: 1.00000 val_loss: 0.38357, val_acc: 0.96667\n",
            "Epoch [1150/10000], loss: 0.33398 acc: 1.00000 val_loss: 0.38199, val_acc: 0.96667\n",
            "Epoch [1160/10000], loss: 0.33230 acc: 1.00000 val_loss: 0.38043, val_acc: 0.96667\n",
            "Epoch [1170/10000], loss: 0.33064 acc: 1.00000 val_loss: 0.37889, val_acc: 0.96667\n",
            "Epoch [1180/10000], loss: 0.32900 acc: 1.00000 val_loss: 0.37737, val_acc: 0.96667\n",
            "Epoch [1190/10000], loss: 0.32737 acc: 1.00000 val_loss: 0.37586, val_acc: 0.96667\n",
            "Epoch [1200/10000], loss: 0.32577 acc: 1.00000 val_loss: 0.37437, val_acc: 0.96667\n",
            "Epoch [1210/10000], loss: 0.32418 acc: 1.00000 val_loss: 0.37290, val_acc: 0.96667\n",
            "Epoch [1220/10000], loss: 0.32260 acc: 1.00000 val_loss: 0.37144, val_acc: 0.96667\n",
            "Epoch [1230/10000], loss: 0.32105 acc: 1.00000 val_loss: 0.37000, val_acc: 0.96667\n",
            "Epoch [1240/10000], loss: 0.31951 acc: 1.00000 val_loss: 0.36857, val_acc: 0.96667\n",
            "Epoch [1250/10000], loss: 0.31799 acc: 1.00000 val_loss: 0.36716, val_acc: 0.96667\n",
            "Epoch [1260/10000], loss: 0.31648 acc: 1.00000 val_loss: 0.36576, val_acc: 0.96667\n",
            "Epoch [1270/10000], loss: 0.31499 acc: 1.00000 val_loss: 0.36437, val_acc: 0.96667\n",
            "Epoch [1280/10000], loss: 0.31351 acc: 1.00000 val_loss: 0.36301, val_acc: 0.96667\n",
            "Epoch [1290/10000], loss: 0.31205 acc: 1.00000 val_loss: 0.36165, val_acc: 0.96667\n",
            "Epoch [1300/10000], loss: 0.31061 acc: 1.00000 val_loss: 0.36031, val_acc: 0.96667\n",
            "Epoch [1310/10000], loss: 0.30918 acc: 1.00000 val_loss: 0.35898, val_acc: 0.96667\n",
            "Epoch [1320/10000], loss: 0.30776 acc: 1.00000 val_loss: 0.35767, val_acc: 0.96667\n",
            "Epoch [1330/10000], loss: 0.30636 acc: 1.00000 val_loss: 0.35637, val_acc: 0.96667\n",
            "Epoch [1340/10000], loss: 0.30498 acc: 1.00000 val_loss: 0.35508, val_acc: 0.96667\n",
            "Epoch [1350/10000], loss: 0.30360 acc: 1.00000 val_loss: 0.35381, val_acc: 0.96667\n",
            "Epoch [1360/10000], loss: 0.30224 acc: 1.00000 val_loss: 0.35255, val_acc: 0.96667\n",
            "Epoch [1370/10000], loss: 0.30090 acc: 1.00000 val_loss: 0.35130, val_acc: 0.96667\n",
            "Epoch [1380/10000], loss: 0.29957 acc: 1.00000 val_loss: 0.35006, val_acc: 0.96667\n",
            "Epoch [1390/10000], loss: 0.29825 acc: 1.00000 val_loss: 0.34884, val_acc: 0.96667\n",
            "Epoch [1400/10000], loss: 0.29694 acc: 1.00000 val_loss: 0.34763, val_acc: 0.96667\n",
            "Epoch [1410/10000], loss: 0.29565 acc: 1.00000 val_loss: 0.34643, val_acc: 0.96667\n",
            "Epoch [1420/10000], loss: 0.29437 acc: 1.00000 val_loss: 0.34524, val_acc: 0.96667\n",
            "Epoch [1430/10000], loss: 0.29310 acc: 1.00000 val_loss: 0.34406, val_acc: 0.96667\n",
            "Epoch [1440/10000], loss: 0.29184 acc: 1.00000 val_loss: 0.34290, val_acc: 0.96667\n",
            "Epoch [1450/10000], loss: 0.29060 acc: 1.00000 val_loss: 0.34174, val_acc: 0.96667\n",
            "Epoch [1460/10000], loss: 0.28937 acc: 1.00000 val_loss: 0.34060, val_acc: 0.96667\n",
            "Epoch [1470/10000], loss: 0.28815 acc: 1.00000 val_loss: 0.33947, val_acc: 0.96667\n",
            "Epoch [1480/10000], loss: 0.28694 acc: 1.00000 val_loss: 0.33834, val_acc: 0.96667\n",
            "Epoch [1490/10000], loss: 0.28574 acc: 1.00000 val_loss: 0.33723, val_acc: 0.96667\n",
            "Epoch [1500/10000], loss: 0.28456 acc: 1.00000 val_loss: 0.33613, val_acc: 0.96667\n",
            "Epoch [1510/10000], loss: 0.28338 acc: 1.00000 val_loss: 0.33504, val_acc: 0.96667\n",
            "Epoch [1520/10000], loss: 0.28222 acc: 1.00000 val_loss: 0.33396, val_acc: 0.96667\n",
            "Epoch [1530/10000], loss: 0.28106 acc: 1.00000 val_loss: 0.33289, val_acc: 0.96667\n",
            "Epoch [1540/10000], loss: 0.27992 acc: 1.00000 val_loss: 0.33183, val_acc: 0.96667\n",
            "Epoch [1550/10000], loss: 0.27879 acc: 1.00000 val_loss: 0.33078, val_acc: 0.96667\n",
            "Epoch [1560/10000], loss: 0.27767 acc: 1.00000 val_loss: 0.32974, val_acc: 0.96667\n",
            "Epoch [1570/10000], loss: 0.27656 acc: 1.00000 val_loss: 0.32871, val_acc: 0.96667\n",
            "Epoch [1580/10000], loss: 0.27545 acc: 1.00000 val_loss: 0.32769, val_acc: 0.96667\n",
            "Epoch [1590/10000], loss: 0.27436 acc: 1.00000 val_loss: 0.32668, val_acc: 0.96667\n",
            "Epoch [1600/10000], loss: 0.27328 acc: 1.00000 val_loss: 0.32568, val_acc: 0.96667\n",
            "Epoch [1610/10000], loss: 0.27221 acc: 1.00000 val_loss: 0.32468, val_acc: 0.96667\n",
            "Epoch [1620/10000], loss: 0.27115 acc: 1.00000 val_loss: 0.32370, val_acc: 0.96667\n",
            "Epoch [1630/10000], loss: 0.27009 acc: 1.00000 val_loss: 0.32272, val_acc: 0.96667\n",
            "Epoch [1640/10000], loss: 0.26905 acc: 1.00000 val_loss: 0.32175, val_acc: 0.96667\n",
            "Epoch [1650/10000], loss: 0.26802 acc: 1.00000 val_loss: 0.32080, val_acc: 0.96667\n",
            "Epoch [1660/10000], loss: 0.26699 acc: 1.00000 val_loss: 0.31985, val_acc: 0.96667\n",
            "Epoch [1670/10000], loss: 0.26597 acc: 1.00000 val_loss: 0.31890, val_acc: 0.96667\n",
            "Epoch [1680/10000], loss: 0.26497 acc: 1.00000 val_loss: 0.31797, val_acc: 0.96667\n",
            "Epoch [1690/10000], loss: 0.26397 acc: 1.00000 val_loss: 0.31704, val_acc: 0.96667\n",
            "Epoch [1700/10000], loss: 0.26298 acc: 1.00000 val_loss: 0.31613, val_acc: 0.96667\n",
            "Epoch [1710/10000], loss: 0.26199 acc: 1.00000 val_loss: 0.31522, val_acc: 0.96667\n",
            "Epoch [1720/10000], loss: 0.26102 acc: 1.00000 val_loss: 0.31431, val_acc: 0.96667\n",
            "Epoch [1730/10000], loss: 0.26005 acc: 1.00000 val_loss: 0.31342, val_acc: 0.96667\n",
            "Epoch [1740/10000], loss: 0.25910 acc: 1.00000 val_loss: 0.31253, val_acc: 0.96667\n",
            "Epoch [1750/10000], loss: 0.25815 acc: 1.00000 val_loss: 0.31165, val_acc: 0.96667\n",
            "Epoch [1760/10000], loss: 0.25721 acc: 1.00000 val_loss: 0.31078, val_acc: 0.96667\n",
            "Epoch [1770/10000], loss: 0.25627 acc: 1.00000 val_loss: 0.30992, val_acc: 0.96667\n",
            "Epoch [1780/10000], loss: 0.25535 acc: 1.00000 val_loss: 0.30906, val_acc: 0.96667\n",
            "Epoch [1790/10000], loss: 0.25443 acc: 1.00000 val_loss: 0.30821, val_acc: 0.96667\n",
            "Epoch [1800/10000], loss: 0.25352 acc: 1.00000 val_loss: 0.30737, val_acc: 0.96667\n",
            "Epoch [1810/10000], loss: 0.25262 acc: 1.00000 val_loss: 0.30653, val_acc: 0.96667\n",
            "Epoch [1820/10000], loss: 0.25172 acc: 1.00000 val_loss: 0.30571, val_acc: 0.96667\n",
            "Epoch [1830/10000], loss: 0.25083 acc: 1.00000 val_loss: 0.30488, val_acc: 0.96667\n",
            "Epoch [1840/10000], loss: 0.24995 acc: 1.00000 val_loss: 0.30407, val_acc: 0.96667\n",
            "Epoch [1850/10000], loss: 0.24908 acc: 1.00000 val_loss: 0.30326, val_acc: 0.96667\n",
            "Epoch [1860/10000], loss: 0.24821 acc: 1.00000 val_loss: 0.30246, val_acc: 0.96667\n",
            "Epoch [1870/10000], loss: 0.24735 acc: 1.00000 val_loss: 0.30166, val_acc: 0.96667\n",
            "Epoch [1880/10000], loss: 0.24650 acc: 1.00000 val_loss: 0.30088, val_acc: 0.96667\n",
            "Epoch [1890/10000], loss: 0.24565 acc: 1.00000 val_loss: 0.30009, val_acc: 0.96667\n",
            "Epoch [1900/10000], loss: 0.24481 acc: 1.00000 val_loss: 0.29932, val_acc: 0.96667\n",
            "Epoch [1910/10000], loss: 0.24398 acc: 1.00000 val_loss: 0.29855, val_acc: 0.96667\n",
            "Epoch [1920/10000], loss: 0.24315 acc: 1.00000 val_loss: 0.29778, val_acc: 0.96667\n",
            "Epoch [1930/10000], loss: 0.24233 acc: 1.00000 val_loss: 0.29702, val_acc: 0.96667\n",
            "Epoch [1940/10000], loss: 0.24152 acc: 1.00000 val_loss: 0.29627, val_acc: 0.96667\n",
            "Epoch [1950/10000], loss: 0.24071 acc: 1.00000 val_loss: 0.29553, val_acc: 0.96667\n",
            "Epoch [1960/10000], loss: 0.23991 acc: 1.00000 val_loss: 0.29479, val_acc: 0.96667\n",
            "Epoch [1970/10000], loss: 0.23911 acc: 1.00000 val_loss: 0.29405, val_acc: 0.96667\n",
            "Epoch [1980/10000], loss: 0.23833 acc: 1.00000 val_loss: 0.29332, val_acc: 0.96667\n",
            "Epoch [1990/10000], loss: 0.23754 acc: 1.00000 val_loss: 0.29260, val_acc: 0.96667\n",
            "Epoch [2000/10000], loss: 0.23677 acc: 1.00000 val_loss: 0.29188, val_acc: 0.96667\n",
            "Epoch [2010/10000], loss: 0.23599 acc: 1.00000 val_loss: 0.29117, val_acc: 0.96667\n",
            "Epoch [2020/10000], loss: 0.23523 acc: 1.00000 val_loss: 0.29047, val_acc: 0.96667\n",
            "Epoch [2030/10000], loss: 0.23447 acc: 1.00000 val_loss: 0.28977, val_acc: 0.96667\n",
            "Epoch [2040/10000], loss: 0.23372 acc: 1.00000 val_loss: 0.28907, val_acc: 0.96667\n",
            "Epoch [2050/10000], loss: 0.23297 acc: 1.00000 val_loss: 0.28838, val_acc: 0.96667\n",
            "Epoch [2060/10000], loss: 0.23223 acc: 1.00000 val_loss: 0.28770, val_acc: 0.96667\n",
            "Epoch [2070/10000], loss: 0.23149 acc: 1.00000 val_loss: 0.28702, val_acc: 0.96667\n",
            "Epoch [2080/10000], loss: 0.23076 acc: 1.00000 val_loss: 0.28634, val_acc: 0.96667\n",
            "Epoch [2090/10000], loss: 0.23003 acc: 1.00000 val_loss: 0.28567, val_acc: 0.96667\n",
            "Epoch [2100/10000], loss: 0.22931 acc: 1.00000 val_loss: 0.28501, val_acc: 0.96667\n",
            "Epoch [2110/10000], loss: 0.22859 acc: 1.00000 val_loss: 0.28435, val_acc: 0.96667\n",
            "Epoch [2120/10000], loss: 0.22788 acc: 1.00000 val_loss: 0.28369, val_acc: 0.96667\n",
            "Epoch [2130/10000], loss: 0.22718 acc: 1.00000 val_loss: 0.28304, val_acc: 0.96667\n",
            "Epoch [2140/10000], loss: 0.22648 acc: 1.00000 val_loss: 0.28240, val_acc: 0.96667\n",
            "Epoch [2150/10000], loss: 0.22578 acc: 1.00000 val_loss: 0.28176, val_acc: 0.96667\n",
            "Epoch [2160/10000], loss: 0.22509 acc: 1.00000 val_loss: 0.28112, val_acc: 0.96667\n",
            "Epoch [2170/10000], loss: 0.22441 acc: 1.00000 val_loss: 0.28049, val_acc: 0.96667\n",
            "Epoch [2180/10000], loss: 0.22373 acc: 1.00000 val_loss: 0.27986, val_acc: 0.96667\n",
            "Epoch [2190/10000], loss: 0.22305 acc: 1.00000 val_loss: 0.27924, val_acc: 0.96667\n",
            "Epoch [2200/10000], loss: 0.22238 acc: 1.00000 val_loss: 0.27862, val_acc: 0.96667\n",
            "Epoch [2210/10000], loss: 0.22171 acc: 1.00000 val_loss: 0.27801, val_acc: 0.96667\n",
            "Epoch [2220/10000], loss: 0.22105 acc: 1.00000 val_loss: 0.27740, val_acc: 0.96667\n",
            "Epoch [2230/10000], loss: 0.22039 acc: 1.00000 val_loss: 0.27680, val_acc: 0.96667\n",
            "Epoch [2240/10000], loss: 0.21974 acc: 1.00000 val_loss: 0.27620, val_acc: 0.96667\n",
            "Epoch [2250/10000], loss: 0.21909 acc: 1.00000 val_loss: 0.27560, val_acc: 0.96667\n",
            "Epoch [2260/10000], loss: 0.21845 acc: 1.00000 val_loss: 0.27501, val_acc: 0.96667\n",
            "Epoch [2270/10000], loss: 0.21781 acc: 1.00000 val_loss: 0.27442, val_acc: 0.96667\n",
            "Epoch [2280/10000], loss: 0.21718 acc: 1.00000 val_loss: 0.27384, val_acc: 0.96667\n",
            "Epoch [2290/10000], loss: 0.21655 acc: 1.00000 val_loss: 0.27326, val_acc: 0.96667\n",
            "Epoch [2300/10000], loss: 0.21592 acc: 1.00000 val_loss: 0.27269, val_acc: 0.96667\n",
            "Epoch [2310/10000], loss: 0.21530 acc: 1.00000 val_loss: 0.27211, val_acc: 0.96667\n",
            "Epoch [2320/10000], loss: 0.21468 acc: 1.00000 val_loss: 0.27155, val_acc: 0.96667\n",
            "Epoch [2330/10000], loss: 0.21407 acc: 1.00000 val_loss: 0.27098, val_acc: 0.96667\n",
            "Epoch [2340/10000], loss: 0.21346 acc: 1.00000 val_loss: 0.27042, val_acc: 0.96667\n",
            "Epoch [2350/10000], loss: 0.21285 acc: 1.00000 val_loss: 0.26987, val_acc: 0.96667\n",
            "Epoch [2360/10000], loss: 0.21225 acc: 1.00000 val_loss: 0.26932, val_acc: 0.96667\n",
            "Epoch [2370/10000], loss: 0.21165 acc: 1.00000 val_loss: 0.26877, val_acc: 0.96667\n",
            "Epoch [2380/10000], loss: 0.21106 acc: 1.00000 val_loss: 0.26822, val_acc: 0.96667\n",
            "Epoch [2390/10000], loss: 0.21047 acc: 1.00000 val_loss: 0.26768, val_acc: 0.96667\n",
            "Epoch [2400/10000], loss: 0.20988 acc: 1.00000 val_loss: 0.26715, val_acc: 0.96667\n",
            "Epoch [2410/10000], loss: 0.20930 acc: 1.00000 val_loss: 0.26661, val_acc: 0.96667\n",
            "Epoch [2420/10000], loss: 0.20872 acc: 1.00000 val_loss: 0.26608, val_acc: 0.96667\n",
            "Epoch [2430/10000], loss: 0.20815 acc: 1.00000 val_loss: 0.26556, val_acc: 0.96667\n",
            "Epoch [2440/10000], loss: 0.20758 acc: 1.00000 val_loss: 0.26503, val_acc: 0.96667\n",
            "Epoch [2450/10000], loss: 0.20701 acc: 1.00000 val_loss: 0.26451, val_acc: 0.96667\n",
            "Epoch [2460/10000], loss: 0.20645 acc: 1.00000 val_loss: 0.26400, val_acc: 0.96667\n",
            "Epoch [2470/10000], loss: 0.20589 acc: 1.00000 val_loss: 0.26349, val_acc: 0.96667\n",
            "Epoch [2480/10000], loss: 0.20533 acc: 1.00000 val_loss: 0.26298, val_acc: 0.96667\n",
            "Epoch [2490/10000], loss: 0.20478 acc: 1.00000 val_loss: 0.26247, val_acc: 0.96667\n",
            "Epoch [2500/10000], loss: 0.20423 acc: 1.00000 val_loss: 0.26197, val_acc: 0.96667\n",
            "Epoch [2510/10000], loss: 0.20369 acc: 1.00000 val_loss: 0.26147, val_acc: 0.96667\n",
            "Epoch [2520/10000], loss: 0.20314 acc: 1.00000 val_loss: 0.26097, val_acc: 0.96667\n",
            "Epoch [2530/10000], loss: 0.20261 acc: 1.00000 val_loss: 0.26048, val_acc: 0.96667\n",
            "Epoch [2540/10000], loss: 0.20207 acc: 1.00000 val_loss: 0.25999, val_acc: 0.96667\n",
            "Epoch [2550/10000], loss: 0.20154 acc: 1.00000 val_loss: 0.25950, val_acc: 0.96667\n",
            "Epoch [2560/10000], loss: 0.20101 acc: 1.00000 val_loss: 0.25902, val_acc: 0.96667\n",
            "Epoch [2570/10000], loss: 0.20048 acc: 1.00000 val_loss: 0.25854, val_acc: 0.96667\n",
            "Epoch [2580/10000], loss: 0.19996 acc: 1.00000 val_loss: 0.25806, val_acc: 0.96667\n",
            "Epoch [2590/10000], loss: 0.19944 acc: 1.00000 val_loss: 0.25759, val_acc: 0.96667\n",
            "Epoch [2600/10000], loss: 0.19893 acc: 1.00000 val_loss: 0.25712, val_acc: 0.96667\n",
            "Epoch [2610/10000], loss: 0.19841 acc: 1.00000 val_loss: 0.25665, val_acc: 0.96667\n",
            "Epoch [2620/10000], loss: 0.19791 acc: 1.00000 val_loss: 0.25618, val_acc: 0.96667\n",
            "Epoch [2630/10000], loss: 0.19740 acc: 1.00000 val_loss: 0.25572, val_acc: 0.96667\n",
            "Epoch [2640/10000], loss: 0.19690 acc: 1.00000 val_loss: 0.25526, val_acc: 0.96667\n",
            "Epoch [2650/10000], loss: 0.19640 acc: 1.00000 val_loss: 0.25481, val_acc: 0.96667\n",
            "Epoch [2660/10000], loss: 0.19590 acc: 1.00000 val_loss: 0.25435, val_acc: 0.96667\n",
            "Epoch [2670/10000], loss: 0.19540 acc: 1.00000 val_loss: 0.25390, val_acc: 0.96667\n",
            "Epoch [2680/10000], loss: 0.19491 acc: 1.00000 val_loss: 0.25345, val_acc: 0.96667\n",
            "Epoch [2690/10000], loss: 0.19442 acc: 1.00000 val_loss: 0.25301, val_acc: 0.96667\n",
            "Epoch [2700/10000], loss: 0.19394 acc: 1.00000 val_loss: 0.25257, val_acc: 0.96667\n",
            "Epoch [2710/10000], loss: 0.19346 acc: 1.00000 val_loss: 0.25213, val_acc: 0.96667\n",
            "Epoch [2720/10000], loss: 0.19298 acc: 1.00000 val_loss: 0.25169, val_acc: 0.96667\n",
            "Epoch [2730/10000], loss: 0.19250 acc: 1.00000 val_loss: 0.25125, val_acc: 0.96667\n",
            "Epoch [2740/10000], loss: 0.19202 acc: 1.00000 val_loss: 0.25082, val_acc: 0.96667\n",
            "Epoch [2750/10000], loss: 0.19155 acc: 1.00000 val_loss: 0.25039, val_acc: 0.96667\n",
            "Epoch [2760/10000], loss: 0.19108 acc: 1.00000 val_loss: 0.24997, val_acc: 0.96667\n",
            "Epoch [2770/10000], loss: 0.19062 acc: 1.00000 val_loss: 0.24954, val_acc: 0.96667\n",
            "Epoch [2780/10000], loss: 0.19016 acc: 1.00000 val_loss: 0.24912, val_acc: 0.96667\n",
            "Epoch [2790/10000], loss: 0.18970 acc: 1.00000 val_loss: 0.24870, val_acc: 0.96667\n",
            "Epoch [2800/10000], loss: 0.18924 acc: 1.00000 val_loss: 0.24828, val_acc: 0.96667\n",
            "Epoch [2810/10000], loss: 0.18878 acc: 1.00000 val_loss: 0.24787, val_acc: 0.96667\n",
            "Epoch [2820/10000], loss: 0.18833 acc: 1.00000 val_loss: 0.24746, val_acc: 0.96667\n",
            "Epoch [2830/10000], loss: 0.18788 acc: 1.00000 val_loss: 0.24705, val_acc: 0.96667\n",
            "Epoch [2840/10000], loss: 0.18743 acc: 1.00000 val_loss: 0.24664, val_acc: 0.96667\n",
            "Epoch [2850/10000], loss: 0.18699 acc: 1.00000 val_loss: 0.24624, val_acc: 0.96667\n",
            "Epoch [2860/10000], loss: 0.18654 acc: 1.00000 val_loss: 0.24584, val_acc: 0.96667\n",
            "Epoch [2870/10000], loss: 0.18610 acc: 1.00000 val_loss: 0.24544, val_acc: 0.96667\n",
            "Epoch [2880/10000], loss: 0.18567 acc: 1.00000 val_loss: 0.24504, val_acc: 0.96667\n",
            "Epoch [2890/10000], loss: 0.18523 acc: 1.00000 val_loss: 0.24464, val_acc: 0.96667\n",
            "Epoch [2900/10000], loss: 0.18480 acc: 1.00000 val_loss: 0.24425, val_acc: 0.96667\n",
            "Epoch [2910/10000], loss: 0.18437 acc: 1.00000 val_loss: 0.24386, val_acc: 0.96667\n",
            "Epoch [2920/10000], loss: 0.18394 acc: 1.00000 val_loss: 0.24347, val_acc: 0.96667\n",
            "Epoch [2930/10000], loss: 0.18352 acc: 1.00000 val_loss: 0.24309, val_acc: 0.96667\n",
            "Epoch [2940/10000], loss: 0.18309 acc: 1.00000 val_loss: 0.24270, val_acc: 0.96667\n",
            "Epoch [2950/10000], loss: 0.18267 acc: 1.00000 val_loss: 0.24232, val_acc: 0.96667\n",
            "Epoch [2960/10000], loss: 0.18225 acc: 1.00000 val_loss: 0.24194, val_acc: 0.96667\n",
            "Epoch [2970/10000], loss: 0.18184 acc: 1.00000 val_loss: 0.24156, val_acc: 0.96667\n",
            "Epoch [2980/10000], loss: 0.18142 acc: 1.00000 val_loss: 0.24119, val_acc: 0.96667\n",
            "Epoch [2990/10000], loss: 0.18101 acc: 1.00000 val_loss: 0.24081, val_acc: 0.96667\n",
            "Epoch [3000/10000], loss: 0.18060 acc: 1.00000 val_loss: 0.24044, val_acc: 0.96667\n",
            "Epoch [3010/10000], loss: 0.18019 acc: 1.00000 val_loss: 0.24008, val_acc: 0.96667\n",
            "Epoch [3020/10000], loss: 0.17979 acc: 1.00000 val_loss: 0.23971, val_acc: 0.96667\n",
            "Epoch [3030/10000], loss: 0.17939 acc: 1.00000 val_loss: 0.23934, val_acc: 0.96667\n",
            "Epoch [3040/10000], loss: 0.17899 acc: 1.00000 val_loss: 0.23898, val_acc: 0.96667\n",
            "Epoch [3050/10000], loss: 0.17859 acc: 1.00000 val_loss: 0.23862, val_acc: 0.96667\n",
            "Epoch [3060/10000], loss: 0.17819 acc: 1.00000 val_loss: 0.23826, val_acc: 0.96667\n",
            "Epoch [3070/10000], loss: 0.17780 acc: 1.00000 val_loss: 0.23790, val_acc: 0.96667\n",
            "Epoch [3080/10000], loss: 0.17740 acc: 1.00000 val_loss: 0.23755, val_acc: 0.96667\n",
            "Epoch [3090/10000], loss: 0.17701 acc: 1.00000 val_loss: 0.23720, val_acc: 0.96667\n",
            "Epoch [3100/10000], loss: 0.17662 acc: 1.00000 val_loss: 0.23685, val_acc: 0.96667\n",
            "Epoch [3110/10000], loss: 0.17624 acc: 1.00000 val_loss: 0.23650, val_acc: 0.96667\n",
            "Epoch [3120/10000], loss: 0.17585 acc: 1.00000 val_loss: 0.23615, val_acc: 0.96667\n",
            "Epoch [3130/10000], loss: 0.17547 acc: 1.00000 val_loss: 0.23580, val_acc: 0.96667\n",
            "Epoch [3140/10000], loss: 0.17509 acc: 1.00000 val_loss: 0.23546, val_acc: 0.96667\n",
            "Epoch [3150/10000], loss: 0.17471 acc: 1.00000 val_loss: 0.23512, val_acc: 0.96667\n",
            "Epoch [3160/10000], loss: 0.17434 acc: 1.00000 val_loss: 0.23478, val_acc: 0.96667\n",
            "Epoch [3170/10000], loss: 0.17396 acc: 1.00000 val_loss: 0.23444, val_acc: 0.96667\n",
            "Epoch [3180/10000], loss: 0.17359 acc: 1.00000 val_loss: 0.23411, val_acc: 0.96667\n",
            "Epoch [3190/10000], loss: 0.17322 acc: 1.00000 val_loss: 0.23377, val_acc: 0.96667\n",
            "Epoch [3200/10000], loss: 0.17285 acc: 1.00000 val_loss: 0.23344, val_acc: 0.96667\n",
            "Epoch [3210/10000], loss: 0.17249 acc: 1.00000 val_loss: 0.23311, val_acc: 0.96667\n",
            "Epoch [3220/10000], loss: 0.17212 acc: 1.00000 val_loss: 0.23278, val_acc: 0.96667\n",
            "Epoch [3230/10000], loss: 0.17176 acc: 1.00000 val_loss: 0.23245, val_acc: 0.96667\n",
            "Epoch [3240/10000], loss: 0.17140 acc: 1.00000 val_loss: 0.23213, val_acc: 0.96667\n",
            "Epoch [3250/10000], loss: 0.17104 acc: 1.00000 val_loss: 0.23180, val_acc: 0.96667\n",
            "Epoch [3260/10000], loss: 0.17068 acc: 1.00000 val_loss: 0.23148, val_acc: 0.96667\n",
            "Epoch [3270/10000], loss: 0.17032 acc: 1.00000 val_loss: 0.23116, val_acc: 0.96667\n",
            "Epoch [3280/10000], loss: 0.16997 acc: 1.00000 val_loss: 0.23084, val_acc: 0.96667\n",
            "Epoch [3290/10000], loss: 0.16962 acc: 1.00000 val_loss: 0.23053, val_acc: 0.96667\n",
            "Epoch [3300/10000], loss: 0.16927 acc: 1.00000 val_loss: 0.23021, val_acc: 0.96667\n",
            "Epoch [3310/10000], loss: 0.16892 acc: 1.00000 val_loss: 0.22990, val_acc: 0.96667\n",
            "Epoch [3320/10000], loss: 0.16857 acc: 1.00000 val_loss: 0.22959, val_acc: 0.96667\n",
            "Epoch [3330/10000], loss: 0.16823 acc: 1.00000 val_loss: 0.22928, val_acc: 0.96667\n",
            "Epoch [3340/10000], loss: 0.16788 acc: 1.00000 val_loss: 0.22897, val_acc: 0.96667\n",
            "Epoch [3350/10000], loss: 0.16754 acc: 1.00000 val_loss: 0.22866, val_acc: 0.96667\n",
            "Epoch [3360/10000], loss: 0.16720 acc: 1.00000 val_loss: 0.22835, val_acc: 0.96667\n",
            "Epoch [3370/10000], loss: 0.16686 acc: 1.00000 val_loss: 0.22805, val_acc: 0.96667\n",
            "Epoch [3380/10000], loss: 0.16653 acc: 1.00000 val_loss: 0.22775, val_acc: 0.96667\n",
            "Epoch [3390/10000], loss: 0.16619 acc: 1.00000 val_loss: 0.22745, val_acc: 0.96667\n",
            "Epoch [3400/10000], loss: 0.16586 acc: 1.00000 val_loss: 0.22715, val_acc: 0.96667\n",
            "Epoch [3410/10000], loss: 0.16553 acc: 1.00000 val_loss: 0.22685, val_acc: 0.96667\n",
            "Epoch [3420/10000], loss: 0.16520 acc: 1.00000 val_loss: 0.22655, val_acc: 0.96667\n",
            "Epoch [3430/10000], loss: 0.16487 acc: 1.00000 val_loss: 0.22626, val_acc: 0.96667\n",
            "Epoch [3440/10000], loss: 0.16454 acc: 1.00000 val_loss: 0.22596, val_acc: 0.96667\n",
            "Epoch [3450/10000], loss: 0.16421 acc: 1.00000 val_loss: 0.22567, val_acc: 0.96667\n",
            "Epoch [3460/10000], loss: 0.16389 acc: 1.00000 val_loss: 0.22538, val_acc: 0.96667\n",
            "Epoch [3470/10000], loss: 0.16357 acc: 1.00000 val_loss: 0.22509, val_acc: 0.96667\n",
            "Epoch [3480/10000], loss: 0.16325 acc: 1.00000 val_loss: 0.22480, val_acc: 0.96667\n",
            "Epoch [3490/10000], loss: 0.16293 acc: 1.00000 val_loss: 0.22452, val_acc: 0.96667\n",
            "Epoch [3500/10000], loss: 0.16261 acc: 1.00000 val_loss: 0.22423, val_acc: 0.96667\n",
            "Epoch [3510/10000], loss: 0.16229 acc: 1.00000 val_loss: 0.22395, val_acc: 0.96667\n",
            "Epoch [3520/10000], loss: 0.16198 acc: 1.00000 val_loss: 0.22367, val_acc: 0.96667\n",
            "Epoch [3530/10000], loss: 0.16166 acc: 1.00000 val_loss: 0.22339, val_acc: 0.96667\n",
            "Epoch [3540/10000], loss: 0.16135 acc: 1.00000 val_loss: 0.22311, val_acc: 0.96667\n",
            "Epoch [3550/10000], loss: 0.16104 acc: 1.00000 val_loss: 0.22283, val_acc: 0.96667\n",
            "Epoch [3560/10000], loss: 0.16073 acc: 1.00000 val_loss: 0.22255, val_acc: 0.96667\n",
            "Epoch [3570/10000], loss: 0.16043 acc: 1.00000 val_loss: 0.22228, val_acc: 0.96667\n",
            "Epoch [3580/10000], loss: 0.16012 acc: 1.00000 val_loss: 0.22200, val_acc: 0.96667\n",
            "Epoch [3590/10000], loss: 0.15981 acc: 1.00000 val_loss: 0.22173, val_acc: 0.96667\n",
            "Epoch [3600/10000], loss: 0.15951 acc: 1.00000 val_loss: 0.22146, val_acc: 0.96667\n",
            "Epoch [3610/10000], loss: 0.15921 acc: 1.00000 val_loss: 0.22119, val_acc: 0.96667\n",
            "Epoch [3620/10000], loss: 0.15891 acc: 1.00000 val_loss: 0.22092, val_acc: 0.96667\n",
            "Epoch [3630/10000], loss: 0.15861 acc: 1.00000 val_loss: 0.22065, val_acc: 0.96667\n",
            "Epoch [3640/10000], loss: 0.15831 acc: 1.00000 val_loss: 0.22039, val_acc: 0.96667\n",
            "Epoch [3650/10000], loss: 0.15801 acc: 1.00000 val_loss: 0.22012, val_acc: 0.96667\n",
            "Epoch [3660/10000], loss: 0.15772 acc: 1.00000 val_loss: 0.21986, val_acc: 0.96667\n",
            "Epoch [3670/10000], loss: 0.15743 acc: 1.00000 val_loss: 0.21960, val_acc: 0.96667\n",
            "Epoch [3680/10000], loss: 0.15713 acc: 1.00000 val_loss: 0.21934, val_acc: 0.96667\n",
            "Epoch [3690/10000], loss: 0.15684 acc: 1.00000 val_loss: 0.21908, val_acc: 0.96667\n",
            "Epoch [3700/10000], loss: 0.15655 acc: 1.00000 val_loss: 0.21882, val_acc: 0.96667\n",
            "Epoch [3710/10000], loss: 0.15626 acc: 1.00000 val_loss: 0.21856, val_acc: 0.96667\n",
            "Epoch [3720/10000], loss: 0.15598 acc: 1.00000 val_loss: 0.21830, val_acc: 0.96667\n",
            "Epoch [3730/10000], loss: 0.15569 acc: 1.00000 val_loss: 0.21805, val_acc: 0.96667\n",
            "Epoch [3740/10000], loss: 0.15540 acc: 1.00000 val_loss: 0.21780, val_acc: 0.96667\n",
            "Epoch [3750/10000], loss: 0.15512 acc: 1.00000 val_loss: 0.21754, val_acc: 0.96667\n",
            "Epoch [3760/10000], loss: 0.15484 acc: 1.00000 val_loss: 0.21729, val_acc: 0.96667\n",
            "Epoch [3770/10000], loss: 0.15456 acc: 1.00000 val_loss: 0.21704, val_acc: 0.96667\n",
            "Epoch [3780/10000], loss: 0.15428 acc: 1.00000 val_loss: 0.21679, val_acc: 0.96667\n",
            "Epoch [3790/10000], loss: 0.15400 acc: 1.00000 val_loss: 0.21655, val_acc: 0.96667\n",
            "Epoch [3800/10000], loss: 0.15372 acc: 1.00000 val_loss: 0.21630, val_acc: 0.96667\n",
            "Epoch [3810/10000], loss: 0.15345 acc: 1.00000 val_loss: 0.21605, val_acc: 0.96667\n",
            "Epoch [3820/10000], loss: 0.15317 acc: 1.00000 val_loss: 0.21581, val_acc: 0.96667\n",
            "Epoch [3830/10000], loss: 0.15290 acc: 1.00000 val_loss: 0.21557, val_acc: 0.96667\n",
            "Epoch [3840/10000], loss: 0.15262 acc: 1.00000 val_loss: 0.21532, val_acc: 0.96667\n",
            "Epoch [3850/10000], loss: 0.15235 acc: 1.00000 val_loss: 0.21508, val_acc: 0.96667\n",
            "Epoch [3860/10000], loss: 0.15208 acc: 1.00000 val_loss: 0.21484, val_acc: 0.96667\n",
            "Epoch [3870/10000], loss: 0.15181 acc: 1.00000 val_loss: 0.21460, val_acc: 0.96667\n",
            "Epoch [3880/10000], loss: 0.15155 acc: 1.00000 val_loss: 0.21436, val_acc: 0.96667\n",
            "Epoch [3890/10000], loss: 0.15128 acc: 1.00000 val_loss: 0.21413, val_acc: 0.96667\n",
            "Epoch [3900/10000], loss: 0.15101 acc: 1.00000 val_loss: 0.21389, val_acc: 0.96667\n",
            "Epoch [3910/10000], loss: 0.15075 acc: 1.00000 val_loss: 0.21366, val_acc: 0.96667\n",
            "Epoch [3920/10000], loss: 0.15049 acc: 1.00000 val_loss: 0.21342, val_acc: 0.96667\n",
            "Epoch [3930/10000], loss: 0.15022 acc: 1.00000 val_loss: 0.21319, val_acc: 0.96667\n",
            "Epoch [3940/10000], loss: 0.14996 acc: 1.00000 val_loss: 0.21296, val_acc: 0.96667\n",
            "Epoch [3950/10000], loss: 0.14970 acc: 1.00000 val_loss: 0.21273, val_acc: 0.96667\n",
            "Epoch [3960/10000], loss: 0.14944 acc: 1.00000 val_loss: 0.21250, val_acc: 0.96667\n",
            "Epoch [3970/10000], loss: 0.14919 acc: 1.00000 val_loss: 0.21227, val_acc: 0.96667\n",
            "Epoch [3980/10000], loss: 0.14893 acc: 1.00000 val_loss: 0.21204, val_acc: 0.96667\n",
            "Epoch [3990/10000], loss: 0.14867 acc: 1.00000 val_loss: 0.21182, val_acc: 0.96667\n",
            "Epoch [4000/10000], loss: 0.14842 acc: 1.00000 val_loss: 0.21159, val_acc: 0.96667\n",
            "Epoch [4010/10000], loss: 0.14816 acc: 1.00000 val_loss: 0.21137, val_acc: 0.96667\n",
            "Epoch [4020/10000], loss: 0.14791 acc: 1.00000 val_loss: 0.21114, val_acc: 0.96667\n",
            "Epoch [4030/10000], loss: 0.14766 acc: 1.00000 val_loss: 0.21092, val_acc: 0.96667\n",
            "Epoch [4040/10000], loss: 0.14741 acc: 1.00000 val_loss: 0.21070, val_acc: 0.96667\n",
            "Epoch [4050/10000], loss: 0.14716 acc: 1.00000 val_loss: 0.21048, val_acc: 0.96667\n",
            "Epoch [4060/10000], loss: 0.14691 acc: 1.00000 val_loss: 0.21026, val_acc: 0.96667\n",
            "Epoch [4070/10000], loss: 0.14667 acc: 1.00000 val_loss: 0.21004, val_acc: 0.96667\n",
            "Epoch [4080/10000], loss: 0.14642 acc: 1.00000 val_loss: 0.20982, val_acc: 0.96667\n",
            "Epoch [4090/10000], loss: 0.14617 acc: 1.00000 val_loss: 0.20961, val_acc: 0.96667\n",
            "Epoch [4100/10000], loss: 0.14593 acc: 1.00000 val_loss: 0.20939, val_acc: 0.96667\n",
            "Epoch [4110/10000], loss: 0.14569 acc: 1.00000 val_loss: 0.20918, val_acc: 0.96667\n",
            "Epoch [4120/10000], loss: 0.14544 acc: 1.00000 val_loss: 0.20896, val_acc: 0.96667\n",
            "Epoch [4130/10000], loss: 0.14520 acc: 1.00000 val_loss: 0.20875, val_acc: 0.96667\n",
            "Epoch [4140/10000], loss: 0.14496 acc: 1.00000 val_loss: 0.20854, val_acc: 0.96667\n",
            "Epoch [4150/10000], loss: 0.14472 acc: 1.00000 val_loss: 0.20833, val_acc: 0.96667\n",
            "Epoch [4160/10000], loss: 0.14448 acc: 1.00000 val_loss: 0.20812, val_acc: 0.96667\n",
            "Epoch [4170/10000], loss: 0.14425 acc: 1.00000 val_loss: 0.20791, val_acc: 0.96667\n",
            "Epoch [4180/10000], loss: 0.14401 acc: 1.00000 val_loss: 0.20770, val_acc: 0.96667\n",
            "Epoch [4190/10000], loss: 0.14377 acc: 1.00000 val_loss: 0.20749, val_acc: 0.96667\n",
            "Epoch [4200/10000], loss: 0.14354 acc: 1.00000 val_loss: 0.20728, val_acc: 0.96667\n",
            "Epoch [4210/10000], loss: 0.14331 acc: 1.00000 val_loss: 0.20708, val_acc: 0.96667\n",
            "Epoch [4220/10000], loss: 0.14307 acc: 1.00000 val_loss: 0.20687, val_acc: 0.96667\n",
            "Epoch [4230/10000], loss: 0.14284 acc: 1.00000 val_loss: 0.20667, val_acc: 0.96667\n",
            "Epoch [4240/10000], loss: 0.14261 acc: 1.00000 val_loss: 0.20647, val_acc: 0.96667\n",
            "Epoch [4250/10000], loss: 0.14238 acc: 1.00000 val_loss: 0.20626, val_acc: 0.96667\n",
            "Epoch [4260/10000], loss: 0.14215 acc: 1.00000 val_loss: 0.20606, val_acc: 0.96667\n",
            "Epoch [4270/10000], loss: 0.14192 acc: 1.00000 val_loss: 0.20586, val_acc: 0.96667\n",
            "Epoch [4280/10000], loss: 0.14170 acc: 1.00000 val_loss: 0.20566, val_acc: 0.96667\n",
            "Epoch [4290/10000], loss: 0.14147 acc: 1.00000 val_loss: 0.20546, val_acc: 0.96667\n",
            "Epoch [4300/10000], loss: 0.14124 acc: 1.00000 val_loss: 0.20526, val_acc: 0.96667\n",
            "Epoch [4310/10000], loss: 0.14102 acc: 1.00000 val_loss: 0.20507, val_acc: 0.96667\n",
            "Epoch [4320/10000], loss: 0.14080 acc: 1.00000 val_loss: 0.20487, val_acc: 0.96667\n",
            "Epoch [4330/10000], loss: 0.14057 acc: 1.00000 val_loss: 0.20467, val_acc: 0.96667\n",
            "Epoch [4340/10000], loss: 0.14035 acc: 1.00000 val_loss: 0.20448, val_acc: 0.96667\n",
            "Epoch [4350/10000], loss: 0.14013 acc: 1.00000 val_loss: 0.20429, val_acc: 0.96667\n",
            "Epoch [4360/10000], loss: 0.13991 acc: 1.00000 val_loss: 0.20409, val_acc: 0.96667\n",
            "Epoch [4370/10000], loss: 0.13969 acc: 1.00000 val_loss: 0.20390, val_acc: 0.96667\n",
            "Epoch [4380/10000], loss: 0.13947 acc: 1.00000 val_loss: 0.20371, val_acc: 0.96667\n",
            "Epoch [4390/10000], loss: 0.13925 acc: 1.00000 val_loss: 0.20352, val_acc: 0.96667\n",
            "Epoch [4400/10000], loss: 0.13904 acc: 1.00000 val_loss: 0.20333, val_acc: 0.96667\n",
            "Epoch [4410/10000], loss: 0.13882 acc: 1.00000 val_loss: 0.20314, val_acc: 0.96667\n",
            "Epoch [4420/10000], loss: 0.13860 acc: 1.00000 val_loss: 0.20295, val_acc: 0.96667\n",
            "Epoch [4430/10000], loss: 0.13839 acc: 1.00000 val_loss: 0.20276, val_acc: 0.96667\n",
            "Epoch [4440/10000], loss: 0.13818 acc: 1.00000 val_loss: 0.20257, val_acc: 0.96667\n",
            "Epoch [4450/10000], loss: 0.13796 acc: 1.00000 val_loss: 0.20239, val_acc: 0.96667\n",
            "Epoch [4460/10000], loss: 0.13775 acc: 1.00000 val_loss: 0.20220, val_acc: 0.96667\n",
            "Epoch [4470/10000], loss: 0.13754 acc: 1.00000 val_loss: 0.20202, val_acc: 0.96667\n",
            "Epoch [4480/10000], loss: 0.13733 acc: 1.00000 val_loss: 0.20183, val_acc: 0.96667\n",
            "Epoch [4490/10000], loss: 0.13712 acc: 1.00000 val_loss: 0.20165, val_acc: 0.96667\n",
            "Epoch [4500/10000], loss: 0.13691 acc: 1.00000 val_loss: 0.20147, val_acc: 0.96667\n",
            "Epoch [4510/10000], loss: 0.13670 acc: 1.00000 val_loss: 0.20128, val_acc: 0.96667\n",
            "Epoch [4520/10000], loss: 0.13650 acc: 1.00000 val_loss: 0.20110, val_acc: 0.96667\n",
            "Epoch [4530/10000], loss: 0.13629 acc: 1.00000 val_loss: 0.20092, val_acc: 0.96667\n",
            "Epoch [4540/10000], loss: 0.13608 acc: 1.00000 val_loss: 0.20074, val_acc: 0.96667\n",
            "Epoch [4550/10000], loss: 0.13588 acc: 1.00000 val_loss: 0.20056, val_acc: 0.96667\n",
            "Epoch [4560/10000], loss: 0.13567 acc: 1.00000 val_loss: 0.20038, val_acc: 0.96667\n",
            "Epoch [4570/10000], loss: 0.13547 acc: 1.00000 val_loss: 0.20021, val_acc: 0.96667\n",
            "Epoch [4580/10000], loss: 0.13527 acc: 1.00000 val_loss: 0.20003, val_acc: 0.96667\n",
            "Epoch [4590/10000], loss: 0.13507 acc: 1.00000 val_loss: 0.19985, val_acc: 0.96667\n",
            "Epoch [4600/10000], loss: 0.13486 acc: 1.00000 val_loss: 0.19968, val_acc: 0.96667\n",
            "Epoch [4610/10000], loss: 0.13466 acc: 1.00000 val_loss: 0.19950, val_acc: 0.96667\n",
            "Epoch [4620/10000], loss: 0.13446 acc: 1.00000 val_loss: 0.19933, val_acc: 0.96667\n",
            "Epoch [4630/10000], loss: 0.13426 acc: 1.00000 val_loss: 0.19916, val_acc: 0.96667\n",
            "Epoch [4640/10000], loss: 0.13407 acc: 1.00000 val_loss: 0.19898, val_acc: 0.96667\n",
            "Epoch [4650/10000], loss: 0.13387 acc: 1.00000 val_loss: 0.19881, val_acc: 0.96667\n",
            "Epoch [4660/10000], loss: 0.13367 acc: 1.00000 val_loss: 0.19864, val_acc: 0.96667\n",
            "Epoch [4670/10000], loss: 0.13348 acc: 1.00000 val_loss: 0.19847, val_acc: 0.96667\n",
            "Epoch [4680/10000], loss: 0.13328 acc: 1.00000 val_loss: 0.19830, val_acc: 0.96667\n",
            "Epoch [4690/10000], loss: 0.13308 acc: 1.00000 val_loss: 0.19813, val_acc: 0.96667\n",
            "Epoch [4700/10000], loss: 0.13289 acc: 1.00000 val_loss: 0.19796, val_acc: 0.96667\n",
            "Epoch [4710/10000], loss: 0.13270 acc: 1.00000 val_loss: 0.19779, val_acc: 0.96667\n",
            "Epoch [4720/10000], loss: 0.13250 acc: 1.00000 val_loss: 0.19762, val_acc: 0.96667\n",
            "Epoch [4730/10000], loss: 0.13231 acc: 1.00000 val_loss: 0.19746, val_acc: 0.96667\n",
            "Epoch [4740/10000], loss: 0.13212 acc: 1.00000 val_loss: 0.19729, val_acc: 0.96667\n",
            "Epoch [4750/10000], loss: 0.13193 acc: 1.00000 val_loss: 0.19712, val_acc: 0.96667\n",
            "Epoch [4760/10000], loss: 0.13174 acc: 1.00000 val_loss: 0.19696, val_acc: 0.96667\n",
            "Epoch [4770/10000], loss: 0.13155 acc: 1.00000 val_loss: 0.19679, val_acc: 0.96667\n",
            "Epoch [4780/10000], loss: 0.13136 acc: 1.00000 val_loss: 0.19663, val_acc: 0.96667\n",
            "Epoch [4790/10000], loss: 0.13117 acc: 1.00000 val_loss: 0.19647, val_acc: 0.96667\n",
            "Epoch [4800/10000], loss: 0.13099 acc: 1.00000 val_loss: 0.19630, val_acc: 0.96667\n",
            "Epoch [4810/10000], loss: 0.13080 acc: 1.00000 val_loss: 0.19614, val_acc: 0.96667\n",
            "Epoch [4820/10000], loss: 0.13061 acc: 1.00000 val_loss: 0.19598, val_acc: 0.96667\n",
            "Epoch [4830/10000], loss: 0.13043 acc: 1.00000 val_loss: 0.19582, val_acc: 0.96667\n",
            "Epoch [4840/10000], loss: 0.13024 acc: 1.00000 val_loss: 0.19566, val_acc: 0.96667\n",
            "Epoch [4850/10000], loss: 0.13006 acc: 1.00000 val_loss: 0.19550, val_acc: 0.96667\n",
            "Epoch [4860/10000], loss: 0.12988 acc: 1.00000 val_loss: 0.19534, val_acc: 0.96667\n",
            "Epoch [4870/10000], loss: 0.12969 acc: 1.00000 val_loss: 0.19518, val_acc: 0.96667\n",
            "Epoch [4880/10000], loss: 0.12951 acc: 1.00000 val_loss: 0.19502, val_acc: 0.96667\n",
            "Epoch [4890/10000], loss: 0.12933 acc: 1.00000 val_loss: 0.19487, val_acc: 0.96667\n",
            "Epoch [4900/10000], loss: 0.12915 acc: 1.00000 val_loss: 0.19471, val_acc: 0.96667\n",
            "Epoch [4910/10000], loss: 0.12897 acc: 1.00000 val_loss: 0.19455, val_acc: 0.96667\n",
            "Epoch [4920/10000], loss: 0.12879 acc: 1.00000 val_loss: 0.19440, val_acc: 0.96667\n",
            "Epoch [4930/10000], loss: 0.12861 acc: 1.00000 val_loss: 0.19424, val_acc: 0.96667\n",
            "Epoch [4940/10000], loss: 0.12843 acc: 1.00000 val_loss: 0.19409, val_acc: 0.96667\n",
            "Epoch [4950/10000], loss: 0.12825 acc: 1.00000 val_loss: 0.19394, val_acc: 0.96667\n",
            "Epoch [4960/10000], loss: 0.12808 acc: 1.00000 val_loss: 0.19378, val_acc: 0.96667\n",
            "Epoch [4970/10000], loss: 0.12790 acc: 1.00000 val_loss: 0.19363, val_acc: 0.96667\n",
            "Epoch [4980/10000], loss: 0.12772 acc: 1.00000 val_loss: 0.19348, val_acc: 0.96667\n",
            "Epoch [4990/10000], loss: 0.12755 acc: 1.00000 val_loss: 0.19333, val_acc: 0.96667\n",
            "Epoch [5000/10000], loss: 0.12737 acc: 1.00000 val_loss: 0.19317, val_acc: 0.96667\n",
            "Epoch [5010/10000], loss: 0.12720 acc: 1.00000 val_loss: 0.19302, val_acc: 0.96667\n",
            "Epoch [5020/10000], loss: 0.12703 acc: 1.00000 val_loss: 0.19287, val_acc: 0.96667\n",
            "Epoch [5030/10000], loss: 0.12685 acc: 1.00000 val_loss: 0.19273, val_acc: 0.96667\n",
            "Epoch [5040/10000], loss: 0.12668 acc: 1.00000 val_loss: 0.19258, val_acc: 0.96667\n",
            "Epoch [5050/10000], loss: 0.12651 acc: 1.00000 val_loss: 0.19243, val_acc: 0.96667\n",
            "Epoch [5060/10000], loss: 0.12634 acc: 1.00000 val_loss: 0.19228, val_acc: 0.96667\n",
            "Epoch [5070/10000], loss: 0.12617 acc: 1.00000 val_loss: 0.19213, val_acc: 0.96667\n",
            "Epoch [5080/10000], loss: 0.12600 acc: 1.00000 val_loss: 0.19199, val_acc: 0.96667\n",
            "Epoch [5090/10000], loss: 0.12583 acc: 1.00000 val_loss: 0.19184, val_acc: 0.96667\n",
            "Epoch [5100/10000], loss: 0.12566 acc: 1.00000 val_loss: 0.19169, val_acc: 0.96667\n",
            "Epoch [5110/10000], loss: 0.12549 acc: 1.00000 val_loss: 0.19155, val_acc: 0.96667\n",
            "Epoch [5120/10000], loss: 0.12532 acc: 1.00000 val_loss: 0.19140, val_acc: 0.96667\n",
            "Epoch [5130/10000], loss: 0.12515 acc: 1.00000 val_loss: 0.19126, val_acc: 0.96667\n",
            "Epoch [5140/10000], loss: 0.12499 acc: 1.00000 val_loss: 0.19112, val_acc: 0.96667\n",
            "Epoch [5150/10000], loss: 0.12482 acc: 1.00000 val_loss: 0.19097, val_acc: 0.96667\n",
            "Epoch [5160/10000], loss: 0.12465 acc: 1.00000 val_loss: 0.19083, val_acc: 0.96667\n",
            "Epoch [5170/10000], loss: 0.12449 acc: 1.00000 val_loss: 0.19069, val_acc: 0.96667\n",
            "Epoch [5180/10000], loss: 0.12432 acc: 1.00000 val_loss: 0.19055, val_acc: 0.96667\n",
            "Epoch [5190/10000], loss: 0.12416 acc: 1.00000 val_loss: 0.19041, val_acc: 0.96667\n",
            "Epoch [5200/10000], loss: 0.12400 acc: 1.00000 val_loss: 0.19027, val_acc: 0.96667\n",
            "Epoch [5210/10000], loss: 0.12383 acc: 1.00000 val_loss: 0.19013, val_acc: 0.96667\n",
            "Epoch [5220/10000], loss: 0.12367 acc: 1.00000 val_loss: 0.18999, val_acc: 0.96667\n",
            "Epoch [5230/10000], loss: 0.12351 acc: 1.00000 val_loss: 0.18985, val_acc: 0.96667\n",
            "Epoch [5240/10000], loss: 0.12335 acc: 1.00000 val_loss: 0.18971, val_acc: 0.96667\n",
            "Epoch [5250/10000], loss: 0.12319 acc: 1.00000 val_loss: 0.18957, val_acc: 0.96667\n",
            "Epoch [5260/10000], loss: 0.12303 acc: 1.00000 val_loss: 0.18943, val_acc: 0.96667\n",
            "Epoch [5270/10000], loss: 0.12287 acc: 1.00000 val_loss: 0.18930, val_acc: 0.96667\n",
            "Epoch [5280/10000], loss: 0.12271 acc: 1.00000 val_loss: 0.18916, val_acc: 0.96667\n",
            "Epoch [5290/10000], loss: 0.12255 acc: 1.00000 val_loss: 0.18902, val_acc: 0.96667\n",
            "Epoch [5300/10000], loss: 0.12239 acc: 1.00000 val_loss: 0.18889, val_acc: 0.96667\n",
            "Epoch [5310/10000], loss: 0.12223 acc: 1.00000 val_loss: 0.18875, val_acc: 0.96667\n",
            "Epoch [5320/10000], loss: 0.12207 acc: 1.00000 val_loss: 0.18862, val_acc: 0.96667\n",
            "Epoch [5330/10000], loss: 0.12192 acc: 1.00000 val_loss: 0.18848, val_acc: 0.96667\n",
            "Epoch [5340/10000], loss: 0.12176 acc: 1.00000 val_loss: 0.18835, val_acc: 0.96667\n",
            "Epoch [5350/10000], loss: 0.12160 acc: 1.00000 val_loss: 0.18821, val_acc: 0.96667\n",
            "Epoch [5360/10000], loss: 0.12145 acc: 1.00000 val_loss: 0.18808, val_acc: 0.96667\n",
            "Epoch [5370/10000], loss: 0.12129 acc: 1.00000 val_loss: 0.18795, val_acc: 0.96667\n",
            "Epoch [5380/10000], loss: 0.12114 acc: 1.00000 val_loss: 0.18782, val_acc: 0.96667\n",
            "Epoch [5390/10000], loss: 0.12099 acc: 1.00000 val_loss: 0.18768, val_acc: 0.96667\n",
            "Epoch [5400/10000], loss: 0.12083 acc: 1.00000 val_loss: 0.18755, val_acc: 0.96667\n",
            "Epoch [5410/10000], loss: 0.12068 acc: 1.00000 val_loss: 0.18742, val_acc: 0.96667\n",
            "Epoch [5420/10000], loss: 0.12053 acc: 1.00000 val_loss: 0.18729, val_acc: 0.96667\n",
            "Epoch [5430/10000], loss: 0.12037 acc: 1.00000 val_loss: 0.18716, val_acc: 0.96667\n",
            "Epoch [5440/10000], loss: 0.12022 acc: 1.00000 val_loss: 0.18703, val_acc: 0.96667\n",
            "Epoch [5450/10000], loss: 0.12007 acc: 1.00000 val_loss: 0.18690, val_acc: 0.96667\n",
            "Epoch [5460/10000], loss: 0.11992 acc: 1.00000 val_loss: 0.18678, val_acc: 0.96667\n",
            "Epoch [5470/10000], loss: 0.11977 acc: 1.00000 val_loss: 0.18665, val_acc: 0.96667\n",
            "Epoch [5480/10000], loss: 0.11962 acc: 1.00000 val_loss: 0.18652, val_acc: 0.96667\n",
            "Epoch [5490/10000], loss: 0.11947 acc: 1.00000 val_loss: 0.18639, val_acc: 0.96667\n",
            "Epoch [5500/10000], loss: 0.11932 acc: 1.00000 val_loss: 0.18627, val_acc: 0.96667\n",
            "Epoch [5510/10000], loss: 0.11918 acc: 1.00000 val_loss: 0.18614, val_acc: 0.96667\n",
            "Epoch [5520/10000], loss: 0.11903 acc: 1.00000 val_loss: 0.18601, val_acc: 0.96667\n",
            "Epoch [5530/10000], loss: 0.11888 acc: 1.00000 val_loss: 0.18589, val_acc: 0.96667\n",
            "Epoch [5540/10000], loss: 0.11873 acc: 1.00000 val_loss: 0.18576, val_acc: 0.96667\n",
            "Epoch [5550/10000], loss: 0.11859 acc: 1.00000 val_loss: 0.18564, val_acc: 0.96667\n",
            "Epoch [5560/10000], loss: 0.11844 acc: 1.00000 val_loss: 0.18551, val_acc: 0.96667\n",
            "Epoch [5570/10000], loss: 0.11830 acc: 1.00000 val_loss: 0.18539, val_acc: 0.96667\n",
            "Epoch [5580/10000], loss: 0.11815 acc: 1.00000 val_loss: 0.18527, val_acc: 0.96667\n",
            "Epoch [5590/10000], loss: 0.11801 acc: 1.00000 val_loss: 0.18514, val_acc: 0.96667\n",
            "Epoch [5600/10000], loss: 0.11786 acc: 1.00000 val_loss: 0.18502, val_acc: 0.96667\n",
            "Epoch [5610/10000], loss: 0.11772 acc: 1.00000 val_loss: 0.18490, val_acc: 0.96667\n",
            "Epoch [5620/10000], loss: 0.11757 acc: 1.00000 val_loss: 0.18478, val_acc: 0.96667\n",
            "Epoch [5630/10000], loss: 0.11743 acc: 1.00000 val_loss: 0.18465, val_acc: 0.96667\n",
            "Epoch [5640/10000], loss: 0.11729 acc: 1.00000 val_loss: 0.18453, val_acc: 0.96667\n",
            "Epoch [5650/10000], loss: 0.11715 acc: 1.00000 val_loss: 0.18441, val_acc: 0.96667\n",
            "Epoch [5660/10000], loss: 0.11701 acc: 1.00000 val_loss: 0.18429, val_acc: 0.96667\n",
            "Epoch [5670/10000], loss: 0.11686 acc: 1.00000 val_loss: 0.18417, val_acc: 0.96667\n",
            "Epoch [5680/10000], loss: 0.11672 acc: 1.00000 val_loss: 0.18405, val_acc: 0.96667\n",
            "Epoch [5690/10000], loss: 0.11658 acc: 1.00000 val_loss: 0.18393, val_acc: 0.96667\n",
            "Epoch [5700/10000], loss: 0.11644 acc: 1.00000 val_loss: 0.18381, val_acc: 0.96667\n",
            "Epoch [5710/10000], loss: 0.11630 acc: 1.00000 val_loss: 0.18370, val_acc: 0.96667\n",
            "Epoch [5720/10000], loss: 0.11616 acc: 1.00000 val_loss: 0.18358, val_acc: 0.96667\n",
            "Epoch [5730/10000], loss: 0.11603 acc: 1.00000 val_loss: 0.18346, val_acc: 0.96667\n",
            "Epoch [5740/10000], loss: 0.11589 acc: 1.00000 val_loss: 0.18334, val_acc: 0.96667\n",
            "Epoch [5750/10000], loss: 0.11575 acc: 1.00000 val_loss: 0.18323, val_acc: 0.96667\n",
            "Epoch [5760/10000], loss: 0.11561 acc: 1.00000 val_loss: 0.18311, val_acc: 0.96667\n",
            "Epoch [5770/10000], loss: 0.11547 acc: 1.00000 val_loss: 0.18299, val_acc: 0.96667\n",
            "Epoch [5780/10000], loss: 0.11534 acc: 1.00000 val_loss: 0.18288, val_acc: 0.96667\n",
            "Epoch [5790/10000], loss: 0.11520 acc: 1.00000 val_loss: 0.18276, val_acc: 0.96667\n",
            "Epoch [5800/10000], loss: 0.11507 acc: 1.00000 val_loss: 0.18265, val_acc: 0.96667\n",
            "Epoch [5810/10000], loss: 0.11493 acc: 1.00000 val_loss: 0.18253, val_acc: 0.96667\n",
            "Epoch [5820/10000], loss: 0.11480 acc: 1.00000 val_loss: 0.18242, val_acc: 0.96667\n",
            "Epoch [5830/10000], loss: 0.11466 acc: 1.00000 val_loss: 0.18231, val_acc: 0.96667\n",
            "Epoch [5840/10000], loss: 0.11453 acc: 1.00000 val_loss: 0.18219, val_acc: 0.96667\n",
            "Epoch [5850/10000], loss: 0.11439 acc: 1.00000 val_loss: 0.18208, val_acc: 0.96667\n",
            "Epoch [5860/10000], loss: 0.11426 acc: 1.00000 val_loss: 0.18197, val_acc: 0.96667\n",
            "Epoch [5870/10000], loss: 0.11413 acc: 1.00000 val_loss: 0.18185, val_acc: 0.96667\n",
            "Epoch [5880/10000], loss: 0.11399 acc: 1.00000 val_loss: 0.18174, val_acc: 0.96667\n",
            "Epoch [5890/10000], loss: 0.11386 acc: 1.00000 val_loss: 0.18163, val_acc: 0.96667\n",
            "Epoch [5900/10000], loss: 0.11373 acc: 1.00000 val_loss: 0.18152, val_acc: 0.96667\n",
            "Epoch [5910/10000], loss: 0.11360 acc: 1.00000 val_loss: 0.18141, val_acc: 0.96667\n",
            "Epoch [5920/10000], loss: 0.11347 acc: 1.00000 val_loss: 0.18130, val_acc: 0.96667\n",
            "Epoch [5930/10000], loss: 0.11333 acc: 1.00000 val_loss: 0.18119, val_acc: 0.96667\n",
            "Epoch [5940/10000], loss: 0.11320 acc: 1.00000 val_loss: 0.18108, val_acc: 0.96667\n",
            "Epoch [5950/10000], loss: 0.11307 acc: 1.00000 val_loss: 0.18097, val_acc: 0.96667\n",
            "Epoch [5960/10000], loss: 0.11294 acc: 1.00000 val_loss: 0.18086, val_acc: 0.96667\n",
            "Epoch [5970/10000], loss: 0.11282 acc: 1.00000 val_loss: 0.18075, val_acc: 0.96667\n",
            "Epoch [5980/10000], loss: 0.11269 acc: 1.00000 val_loss: 0.18064, val_acc: 0.96667\n",
            "Epoch [5990/10000], loss: 0.11256 acc: 1.00000 val_loss: 0.18053, val_acc: 0.96667\n",
            "Epoch [6000/10000], loss: 0.11243 acc: 1.00000 val_loss: 0.18042, val_acc: 0.96667\n",
            "Epoch [6010/10000], loss: 0.11230 acc: 1.00000 val_loss: 0.18031, val_acc: 0.96667\n",
            "Epoch [6020/10000], loss: 0.11217 acc: 1.00000 val_loss: 0.18021, val_acc: 0.96667\n",
            "Epoch [6030/10000], loss: 0.11205 acc: 1.00000 val_loss: 0.18010, val_acc: 0.96667\n",
            "Epoch [6040/10000], loss: 0.11192 acc: 1.00000 val_loss: 0.17999, val_acc: 0.96667\n",
            "Epoch [6050/10000], loss: 0.11179 acc: 1.00000 val_loss: 0.17989, val_acc: 0.96667\n",
            "Epoch [6060/10000], loss: 0.11167 acc: 1.00000 val_loss: 0.17978, val_acc: 0.96667\n",
            "Epoch [6070/10000], loss: 0.11154 acc: 1.00000 val_loss: 0.17968, val_acc: 0.96667\n",
            "Epoch [6080/10000], loss: 0.11142 acc: 1.00000 val_loss: 0.17957, val_acc: 0.96667\n",
            "Epoch [6090/10000], loss: 0.11129 acc: 1.00000 val_loss: 0.17947, val_acc: 0.96667\n",
            "Epoch [6100/10000], loss: 0.11117 acc: 1.00000 val_loss: 0.17936, val_acc: 0.96667\n",
            "Epoch [6110/10000], loss: 0.11104 acc: 1.00000 val_loss: 0.17926, val_acc: 0.96667\n",
            "Epoch [6120/10000], loss: 0.11092 acc: 1.00000 val_loss: 0.17915, val_acc: 0.96667\n",
            "Epoch [6130/10000], loss: 0.11079 acc: 1.00000 val_loss: 0.17905, val_acc: 0.96667\n",
            "Epoch [6140/10000], loss: 0.11067 acc: 1.00000 val_loss: 0.17894, val_acc: 0.96667\n",
            "Epoch [6150/10000], loss: 0.11055 acc: 1.00000 val_loss: 0.17884, val_acc: 0.96667\n",
            "Epoch [6160/10000], loss: 0.11043 acc: 1.00000 val_loss: 0.17874, val_acc: 0.96667\n",
            "Epoch [6170/10000], loss: 0.11030 acc: 1.00000 val_loss: 0.17864, val_acc: 0.96667\n",
            "Epoch [6180/10000], loss: 0.11018 acc: 1.00000 val_loss: 0.17853, val_acc: 0.96667\n",
            "Epoch [6190/10000], loss: 0.11006 acc: 1.00000 val_loss: 0.17843, val_acc: 0.96667\n",
            "Epoch [6200/10000], loss: 0.10994 acc: 1.00000 val_loss: 0.17833, val_acc: 0.96667\n",
            "Epoch [6210/10000], loss: 0.10982 acc: 1.00000 val_loss: 0.17823, val_acc: 0.96667\n",
            "Epoch [6220/10000], loss: 0.10970 acc: 1.00000 val_loss: 0.17813, val_acc: 0.96667\n",
            "Epoch [6230/10000], loss: 0.10958 acc: 1.00000 val_loss: 0.17803, val_acc: 0.96667\n",
            "Epoch [6240/10000], loss: 0.10946 acc: 1.00000 val_loss: 0.17793, val_acc: 0.96667\n",
            "Epoch [6250/10000], loss: 0.10934 acc: 1.00000 val_loss: 0.17783, val_acc: 0.96667\n",
            "Epoch [6260/10000], loss: 0.10922 acc: 1.00000 val_loss: 0.17773, val_acc: 0.96667\n",
            "Epoch [6270/10000], loss: 0.10910 acc: 1.00000 val_loss: 0.17763, val_acc: 0.96667\n",
            "Epoch [6280/10000], loss: 0.10898 acc: 1.00000 val_loss: 0.17753, val_acc: 0.96667\n",
            "Epoch [6290/10000], loss: 0.10886 acc: 1.00000 val_loss: 0.17743, val_acc: 0.96667\n",
            "Epoch [6300/10000], loss: 0.10874 acc: 1.00000 val_loss: 0.17733, val_acc: 0.96667\n",
            "Epoch [6310/10000], loss: 0.10863 acc: 1.00000 val_loss: 0.17723, val_acc: 0.96667\n",
            "Epoch [6320/10000], loss: 0.10851 acc: 1.00000 val_loss: 0.17713, val_acc: 0.96667\n",
            "Epoch [6330/10000], loss: 0.10839 acc: 1.00000 val_loss: 0.17704, val_acc: 0.96667\n",
            "Epoch [6340/10000], loss: 0.10828 acc: 1.00000 val_loss: 0.17694, val_acc: 0.96667\n",
            "Epoch [6350/10000], loss: 0.10816 acc: 1.00000 val_loss: 0.17684, val_acc: 0.96667\n",
            "Epoch [6360/10000], loss: 0.10804 acc: 1.00000 val_loss: 0.17675, val_acc: 0.96667\n",
            "Epoch [6370/10000], loss: 0.10793 acc: 1.00000 val_loss: 0.17665, val_acc: 0.96667\n",
            "Epoch [6380/10000], loss: 0.10781 acc: 1.00000 val_loss: 0.17655, val_acc: 0.96667\n",
            "Epoch [6390/10000], loss: 0.10770 acc: 1.00000 val_loss: 0.17646, val_acc: 0.96667\n",
            "Epoch [6400/10000], loss: 0.10758 acc: 1.00000 val_loss: 0.17636, val_acc: 0.96667\n",
            "Epoch [6410/10000], loss: 0.10747 acc: 1.00000 val_loss: 0.17627, val_acc: 0.96667\n",
            "Epoch [6420/10000], loss: 0.10735 acc: 1.00000 val_loss: 0.17617, val_acc: 0.96667\n",
            "Epoch [6430/10000], loss: 0.10724 acc: 1.00000 val_loss: 0.17608, val_acc: 0.96667\n",
            "Epoch [6440/10000], loss: 0.10713 acc: 1.00000 val_loss: 0.17598, val_acc: 0.96667\n",
            "Epoch [6450/10000], loss: 0.10701 acc: 1.00000 val_loss: 0.17589, val_acc: 0.96667\n",
            "Epoch [6460/10000], loss: 0.10690 acc: 1.00000 val_loss: 0.17579, val_acc: 0.96667\n",
            "Epoch [6470/10000], loss: 0.10679 acc: 1.00000 val_loss: 0.17570, val_acc: 0.96667\n",
            "Epoch [6480/10000], loss: 0.10667 acc: 1.00000 val_loss: 0.17561, val_acc: 0.96667\n",
            "Epoch [6490/10000], loss: 0.10656 acc: 1.00000 val_loss: 0.17551, val_acc: 0.96667\n",
            "Epoch [6500/10000], loss: 0.10645 acc: 1.00000 val_loss: 0.17542, val_acc: 0.96667\n",
            "Epoch [6510/10000], loss: 0.10634 acc: 1.00000 val_loss: 0.17533, val_acc: 0.96667\n",
            "Epoch [6520/10000], loss: 0.10623 acc: 1.00000 val_loss: 0.17523, val_acc: 0.96667\n",
            "Epoch [6530/10000], loss: 0.10612 acc: 1.00000 val_loss: 0.17514, val_acc: 0.96667\n",
            "Epoch [6540/10000], loss: 0.10600 acc: 1.00000 val_loss: 0.17505, val_acc: 0.96667\n",
            "Epoch [6550/10000], loss: 0.10589 acc: 1.00000 val_loss: 0.17496, val_acc: 0.96667\n",
            "Epoch [6560/10000], loss: 0.10578 acc: 1.00000 val_loss: 0.17487, val_acc: 0.96667\n",
            "Epoch [6570/10000], loss: 0.10567 acc: 1.00000 val_loss: 0.17478, val_acc: 0.96667\n",
            "Epoch [6580/10000], loss: 0.10556 acc: 1.00000 val_loss: 0.17468, val_acc: 0.96667\n",
            "Epoch [6590/10000], loss: 0.10546 acc: 1.00000 val_loss: 0.17459, val_acc: 0.96667\n",
            "Epoch [6600/10000], loss: 0.10535 acc: 1.00000 val_loss: 0.17450, val_acc: 0.96667\n",
            "Epoch [6610/10000], loss: 0.10524 acc: 1.00000 val_loss: 0.17441, val_acc: 0.96667\n",
            "Epoch [6620/10000], loss: 0.10513 acc: 1.00000 val_loss: 0.17432, val_acc: 0.96667\n",
            "Epoch [6630/10000], loss: 0.10502 acc: 1.00000 val_loss: 0.17423, val_acc: 0.96667\n",
            "Epoch [6640/10000], loss: 0.10491 acc: 1.00000 val_loss: 0.17414, val_acc: 0.96667\n",
            "Epoch [6650/10000], loss: 0.10481 acc: 1.00000 val_loss: 0.17406, val_acc: 0.96667\n",
            "Epoch [6660/10000], loss: 0.10470 acc: 1.00000 val_loss: 0.17397, val_acc: 0.96667\n",
            "Epoch [6670/10000], loss: 0.10459 acc: 1.00000 val_loss: 0.17388, val_acc: 0.96667\n",
            "Epoch [6680/10000], loss: 0.10448 acc: 1.00000 val_loss: 0.17379, val_acc: 0.96667\n",
            "Epoch [6690/10000], loss: 0.10438 acc: 1.00000 val_loss: 0.17370, val_acc: 0.96667\n",
            "Epoch [6700/10000], loss: 0.10427 acc: 1.00000 val_loss: 0.17361, val_acc: 0.96667\n",
            "Epoch [6710/10000], loss: 0.10417 acc: 1.00000 val_loss: 0.17353, val_acc: 0.96667\n",
            "Epoch [6720/10000], loss: 0.10406 acc: 1.00000 val_loss: 0.17344, val_acc: 0.96667\n",
            "Epoch [6730/10000], loss: 0.10395 acc: 1.00000 val_loss: 0.17335, val_acc: 0.96667\n",
            "Epoch [6740/10000], loss: 0.10385 acc: 1.00000 val_loss: 0.17326, val_acc: 0.96667\n",
            "Epoch [6750/10000], loss: 0.10374 acc: 1.00000 val_loss: 0.17318, val_acc: 0.96667\n",
            "Epoch [6760/10000], loss: 0.10364 acc: 1.00000 val_loss: 0.17309, val_acc: 0.96667\n",
            "Epoch [6770/10000], loss: 0.10354 acc: 1.00000 val_loss: 0.17301, val_acc: 0.96667\n",
            "Epoch [6780/10000], loss: 0.10343 acc: 1.00000 val_loss: 0.17292, val_acc: 0.96667\n",
            "Epoch [6790/10000], loss: 0.10333 acc: 1.00000 val_loss: 0.17283, val_acc: 0.96667\n",
            "Epoch [6800/10000], loss: 0.10322 acc: 1.00000 val_loss: 0.17275, val_acc: 0.96667\n",
            "Epoch [6810/10000], loss: 0.10312 acc: 1.00000 val_loss: 0.17266, val_acc: 0.96667\n",
            "Epoch [6820/10000], loss: 0.10302 acc: 1.00000 val_loss: 0.17258, val_acc: 0.96667\n",
            "Epoch [6830/10000], loss: 0.10292 acc: 1.00000 val_loss: 0.17249, val_acc: 0.96667\n",
            "Epoch [6840/10000], loss: 0.10281 acc: 1.00000 val_loss: 0.17241, val_acc: 0.96667\n",
            "Epoch [6850/10000], loss: 0.10271 acc: 1.00000 val_loss: 0.17233, val_acc: 0.96667\n",
            "Epoch [6860/10000], loss: 0.10261 acc: 1.00000 val_loss: 0.17224, val_acc: 0.96667\n",
            "Epoch [6870/10000], loss: 0.10251 acc: 1.00000 val_loss: 0.17216, val_acc: 0.96667\n",
            "Epoch [6880/10000], loss: 0.10241 acc: 1.00000 val_loss: 0.17207, val_acc: 0.96667\n",
            "Epoch [6890/10000], loss: 0.10230 acc: 1.00000 val_loss: 0.17199, val_acc: 0.96667\n",
            "Epoch [6900/10000], loss: 0.10220 acc: 1.00000 val_loss: 0.17191, val_acc: 0.96667\n",
            "Epoch [6910/10000], loss: 0.10210 acc: 1.00000 val_loss: 0.17182, val_acc: 0.96667\n",
            "Epoch [6920/10000], loss: 0.10200 acc: 1.00000 val_loss: 0.17174, val_acc: 0.96667\n",
            "Epoch [6930/10000], loss: 0.10190 acc: 1.00000 val_loss: 0.17166, val_acc: 0.96667\n",
            "Epoch [6940/10000], loss: 0.10180 acc: 1.00000 val_loss: 0.17158, val_acc: 0.96667\n",
            "Epoch [6950/10000], loss: 0.10170 acc: 1.00000 val_loss: 0.17150, val_acc: 0.96667\n",
            "Epoch [6960/10000], loss: 0.10160 acc: 1.00000 val_loss: 0.17141, val_acc: 0.96667\n",
            "Epoch [6970/10000], loss: 0.10150 acc: 1.00000 val_loss: 0.17133, val_acc: 0.96667\n",
            "Epoch [6980/10000], loss: 0.10140 acc: 1.00000 val_loss: 0.17125, val_acc: 0.96667\n",
            "Epoch [6990/10000], loss: 0.10130 acc: 1.00000 val_loss: 0.17117, val_acc: 0.96667\n",
            "Epoch [7000/10000], loss: 0.10121 acc: 1.00000 val_loss: 0.17109, val_acc: 0.96667\n",
            "Epoch [7010/10000], loss: 0.10111 acc: 1.00000 val_loss: 0.17101, val_acc: 0.96667\n",
            "Epoch [7020/10000], loss: 0.10101 acc: 1.00000 val_loss: 0.17093, val_acc: 0.96667\n",
            "Epoch [7030/10000], loss: 0.10091 acc: 1.00000 val_loss: 0.17085, val_acc: 0.96667\n",
            "Epoch [7040/10000], loss: 0.10081 acc: 1.00000 val_loss: 0.17077, val_acc: 0.96667\n",
            "Epoch [7050/10000], loss: 0.10072 acc: 1.00000 val_loss: 0.17069, val_acc: 0.96667\n",
            "Epoch [7060/10000], loss: 0.10062 acc: 1.00000 val_loss: 0.17061, val_acc: 0.96667\n",
            "Epoch [7070/10000], loss: 0.10052 acc: 1.00000 val_loss: 0.17053, val_acc: 0.96667\n",
            "Epoch [7080/10000], loss: 0.10043 acc: 1.00000 val_loss: 0.17045, val_acc: 0.96667\n",
            "Epoch [7090/10000], loss: 0.10033 acc: 1.00000 val_loss: 0.17037, val_acc: 0.96667\n",
            "Epoch [7100/10000], loss: 0.10023 acc: 1.00000 val_loss: 0.17029, val_acc: 0.96667\n",
            "Epoch [7110/10000], loss: 0.10014 acc: 1.00000 val_loss: 0.17021, val_acc: 0.96667\n",
            "Epoch [7120/10000], loss: 0.10004 acc: 1.00000 val_loss: 0.17014, val_acc: 0.96667\n",
            "Epoch [7130/10000], loss: 0.09995 acc: 1.00000 val_loss: 0.17006, val_acc: 0.96667\n",
            "Epoch [7140/10000], loss: 0.09985 acc: 1.00000 val_loss: 0.16998, val_acc: 0.96667\n",
            "Epoch [7150/10000], loss: 0.09976 acc: 1.00000 val_loss: 0.16990, val_acc: 0.96667\n",
            "Epoch [7160/10000], loss: 0.09966 acc: 1.00000 val_loss: 0.16982, val_acc: 0.96667\n",
            "Epoch [7170/10000], loss: 0.09957 acc: 1.00000 val_loss: 0.16975, val_acc: 0.96667\n",
            "Epoch [7180/10000], loss: 0.09947 acc: 1.00000 val_loss: 0.16967, val_acc: 0.96667\n",
            "Epoch [7190/10000], loss: 0.09938 acc: 1.00000 val_loss: 0.16959, val_acc: 0.96667\n",
            "Epoch [7200/10000], loss: 0.09928 acc: 1.00000 val_loss: 0.16952, val_acc: 0.96667\n",
            "Epoch [7210/10000], loss: 0.09919 acc: 1.00000 val_loss: 0.16944, val_acc: 0.96667\n",
            "Epoch [7220/10000], loss: 0.09910 acc: 1.00000 val_loss: 0.16936, val_acc: 0.96667\n",
            "Epoch [7230/10000], loss: 0.09900 acc: 1.00000 val_loss: 0.16929, val_acc: 0.96667\n",
            "Epoch [7240/10000], loss: 0.09891 acc: 1.00000 val_loss: 0.16921, val_acc: 0.96667\n",
            "Epoch [7250/10000], loss: 0.09882 acc: 1.00000 val_loss: 0.16914, val_acc: 0.96667\n",
            "Epoch [7260/10000], loss: 0.09873 acc: 1.00000 val_loss: 0.16906, val_acc: 0.96667\n",
            "Epoch [7270/10000], loss: 0.09863 acc: 1.00000 val_loss: 0.16899, val_acc: 0.96667\n",
            "Epoch [7280/10000], loss: 0.09854 acc: 1.00000 val_loss: 0.16891, val_acc: 0.96667\n",
            "Epoch [7290/10000], loss: 0.09845 acc: 1.00000 val_loss: 0.16884, val_acc: 0.96667\n",
            "Epoch [7300/10000], loss: 0.09836 acc: 1.00000 val_loss: 0.16876, val_acc: 0.96667\n",
            "Epoch [7310/10000], loss: 0.09827 acc: 1.00000 val_loss: 0.16869, val_acc: 0.96667\n",
            "Epoch [7320/10000], loss: 0.09817 acc: 1.00000 val_loss: 0.16861, val_acc: 0.96667\n",
            "Epoch [7330/10000], loss: 0.09808 acc: 1.00000 val_loss: 0.16854, val_acc: 0.96667\n",
            "Epoch [7340/10000], loss: 0.09799 acc: 1.00000 val_loss: 0.16846, val_acc: 0.96667\n",
            "Epoch [7350/10000], loss: 0.09790 acc: 1.00000 val_loss: 0.16839, val_acc: 0.96667\n",
            "Epoch [7360/10000], loss: 0.09781 acc: 1.00000 val_loss: 0.16832, val_acc: 0.96667\n",
            "Epoch [7370/10000], loss: 0.09772 acc: 1.00000 val_loss: 0.16824, val_acc: 0.96667\n",
            "Epoch [7380/10000], loss: 0.09763 acc: 1.00000 val_loss: 0.16817, val_acc: 0.96667\n",
            "Epoch [7390/10000], loss: 0.09754 acc: 1.00000 val_loss: 0.16810, val_acc: 0.96667\n",
            "Epoch [7400/10000], loss: 0.09745 acc: 1.00000 val_loss: 0.16802, val_acc: 0.96667\n",
            "Epoch [7410/10000], loss: 0.09736 acc: 1.00000 val_loss: 0.16795, val_acc: 0.96667\n",
            "Epoch [7420/10000], loss: 0.09727 acc: 1.00000 val_loss: 0.16788, val_acc: 0.96667\n",
            "Epoch [7430/10000], loss: 0.09718 acc: 1.00000 val_loss: 0.16781, val_acc: 0.96667\n",
            "Epoch [7440/10000], loss: 0.09710 acc: 1.00000 val_loss: 0.16774, val_acc: 0.96667\n",
            "Epoch [7450/10000], loss: 0.09701 acc: 1.00000 val_loss: 0.16766, val_acc: 0.96667\n",
            "Epoch [7460/10000], loss: 0.09692 acc: 1.00000 val_loss: 0.16759, val_acc: 0.96667\n",
            "Epoch [7470/10000], loss: 0.09683 acc: 1.00000 val_loss: 0.16752, val_acc: 0.96667\n",
            "Epoch [7480/10000], loss: 0.09674 acc: 1.00000 val_loss: 0.16745, val_acc: 0.96667\n",
            "Epoch [7490/10000], loss: 0.09665 acc: 1.00000 val_loss: 0.16738, val_acc: 0.96667\n",
            "Epoch [7500/10000], loss: 0.09657 acc: 1.00000 val_loss: 0.16731, val_acc: 0.96667\n",
            "Epoch [7510/10000], loss: 0.09648 acc: 1.00000 val_loss: 0.16724, val_acc: 0.96667\n",
            "Epoch [7520/10000], loss: 0.09639 acc: 1.00000 val_loss: 0.16717, val_acc: 0.96667\n",
            "Epoch [7530/10000], loss: 0.09631 acc: 1.00000 val_loss: 0.16710, val_acc: 0.96667\n",
            "Epoch [7540/10000], loss: 0.09622 acc: 1.00000 val_loss: 0.16703, val_acc: 0.96667\n",
            "Epoch [7550/10000], loss: 0.09613 acc: 1.00000 val_loss: 0.16696, val_acc: 0.96667\n",
            "Epoch [7560/10000], loss: 0.09605 acc: 1.00000 val_loss: 0.16689, val_acc: 0.96667\n",
            "Epoch [7570/10000], loss: 0.09596 acc: 1.00000 val_loss: 0.16682, val_acc: 0.96667\n",
            "Epoch [7580/10000], loss: 0.09587 acc: 1.00000 val_loss: 0.16675, val_acc: 0.96667\n",
            "Epoch [7590/10000], loss: 0.09579 acc: 1.00000 val_loss: 0.16668, val_acc: 0.96667\n",
            "Epoch [7600/10000], loss: 0.09570 acc: 1.00000 val_loss: 0.16661, val_acc: 0.96667\n",
            "Epoch [7610/10000], loss: 0.09562 acc: 1.00000 val_loss: 0.16654, val_acc: 0.96667\n",
            "Epoch [7620/10000], loss: 0.09553 acc: 1.00000 val_loss: 0.16647, val_acc: 0.96667\n",
            "Epoch [7630/10000], loss: 0.09545 acc: 1.00000 val_loss: 0.16640, val_acc: 0.96667\n",
            "Epoch [7640/10000], loss: 0.09536 acc: 1.00000 val_loss: 0.16633, val_acc: 0.96667\n",
            "Epoch [7650/10000], loss: 0.09528 acc: 1.00000 val_loss: 0.16626, val_acc: 0.96667\n",
            "Epoch [7660/10000], loss: 0.09519 acc: 1.00000 val_loss: 0.16620, val_acc: 0.96667\n",
            "Epoch [7670/10000], loss: 0.09511 acc: 1.00000 val_loss: 0.16613, val_acc: 0.96667\n",
            "Epoch [7680/10000], loss: 0.09502 acc: 1.00000 val_loss: 0.16606, val_acc: 0.96667\n",
            "Epoch [7690/10000], loss: 0.09494 acc: 1.00000 val_loss: 0.16599, val_acc: 0.96667\n",
            "Epoch [7700/10000], loss: 0.09486 acc: 1.00000 val_loss: 0.16593, val_acc: 0.96667\n",
            "Epoch [7710/10000], loss: 0.09477 acc: 1.00000 val_loss: 0.16586, val_acc: 0.96667\n",
            "Epoch [7720/10000], loss: 0.09469 acc: 1.00000 val_loss: 0.16579, val_acc: 0.96667\n",
            "Epoch [7730/10000], loss: 0.09461 acc: 1.00000 val_loss: 0.16572, val_acc: 0.96667\n",
            "Epoch [7740/10000], loss: 0.09452 acc: 1.00000 val_loss: 0.16566, val_acc: 0.96667\n",
            "Epoch [7750/10000], loss: 0.09444 acc: 1.00000 val_loss: 0.16559, val_acc: 0.96667\n",
            "Epoch [7760/10000], loss: 0.09436 acc: 1.00000 val_loss: 0.16552, val_acc: 0.96667\n",
            "Epoch [7770/10000], loss: 0.09428 acc: 1.00000 val_loss: 0.16546, val_acc: 0.96667\n",
            "Epoch [7780/10000], loss: 0.09419 acc: 1.00000 val_loss: 0.16539, val_acc: 0.96667\n",
            "Epoch [7790/10000], loss: 0.09411 acc: 1.00000 val_loss: 0.16533, val_acc: 0.96667\n",
            "Epoch [7800/10000], loss: 0.09403 acc: 1.00000 val_loss: 0.16526, val_acc: 0.96667\n",
            "Epoch [7810/10000], loss: 0.09395 acc: 1.00000 val_loss: 0.16519, val_acc: 0.96667\n",
            "Epoch [7820/10000], loss: 0.09387 acc: 1.00000 val_loss: 0.16513, val_acc: 0.96667\n",
            "Epoch [7830/10000], loss: 0.09378 acc: 1.00000 val_loss: 0.16506, val_acc: 0.96667\n",
            "Epoch [7840/10000], loss: 0.09370 acc: 1.00000 val_loss: 0.16500, val_acc: 0.96667\n",
            "Epoch [7850/10000], loss: 0.09362 acc: 1.00000 val_loss: 0.16493, val_acc: 0.96667\n",
            "Epoch [7860/10000], loss: 0.09354 acc: 1.00000 val_loss: 0.16487, val_acc: 0.96667\n",
            "Epoch [7870/10000], loss: 0.09346 acc: 1.00000 val_loss: 0.16480, val_acc: 0.96667\n",
            "Epoch [7880/10000], loss: 0.09338 acc: 1.00000 val_loss: 0.16474, val_acc: 0.96667\n",
            "Epoch [7890/10000], loss: 0.09330 acc: 1.00000 val_loss: 0.16468, val_acc: 0.96667\n",
            "Epoch [7900/10000], loss: 0.09322 acc: 1.00000 val_loss: 0.16461, val_acc: 0.96667\n",
            "Epoch [7910/10000], loss: 0.09314 acc: 1.00000 val_loss: 0.16455, val_acc: 0.96667\n",
            "Epoch [7920/10000], loss: 0.09306 acc: 1.00000 val_loss: 0.16448, val_acc: 0.96667\n",
            "Epoch [7930/10000], loss: 0.09298 acc: 1.00000 val_loss: 0.16442, val_acc: 0.96667\n",
            "Epoch [7940/10000], loss: 0.09290 acc: 1.00000 val_loss: 0.16436, val_acc: 0.96667\n",
            "Epoch [7950/10000], loss: 0.09282 acc: 1.00000 val_loss: 0.16429, val_acc: 0.96667\n",
            "Epoch [7960/10000], loss: 0.09274 acc: 1.00000 val_loss: 0.16423, val_acc: 0.96667\n",
            "Epoch [7970/10000], loss: 0.09266 acc: 1.00000 val_loss: 0.16417, val_acc: 0.96667\n",
            "Epoch [7980/10000], loss: 0.09259 acc: 1.00000 val_loss: 0.16410, val_acc: 0.96667\n",
            "Epoch [7990/10000], loss: 0.09251 acc: 1.00000 val_loss: 0.16404, val_acc: 0.96667\n",
            "Epoch [8000/10000], loss: 0.09243 acc: 1.00000 val_loss: 0.16398, val_acc: 0.96667\n",
            "Epoch [8010/10000], loss: 0.09235 acc: 1.00000 val_loss: 0.16392, val_acc: 0.96667\n",
            "Epoch [8020/10000], loss: 0.09227 acc: 1.00000 val_loss: 0.16385, val_acc: 0.96667\n",
            "Epoch [8030/10000], loss: 0.09219 acc: 1.00000 val_loss: 0.16379, val_acc: 0.96667\n",
            "Epoch [8040/10000], loss: 0.09212 acc: 1.00000 val_loss: 0.16373, val_acc: 0.96667\n",
            "Epoch [8050/10000], loss: 0.09204 acc: 1.00000 val_loss: 0.16367, val_acc: 0.96667\n",
            "Epoch [8060/10000], loss: 0.09196 acc: 1.00000 val_loss: 0.16361, val_acc: 0.96667\n",
            "Epoch [8070/10000], loss: 0.09188 acc: 1.00000 val_loss: 0.16354, val_acc: 0.96667\n",
            "Epoch [8080/10000], loss: 0.09181 acc: 1.00000 val_loss: 0.16348, val_acc: 0.96667\n",
            "Epoch [8090/10000], loss: 0.09173 acc: 1.00000 val_loss: 0.16342, val_acc: 0.96667\n",
            "Epoch [8100/10000], loss: 0.09165 acc: 1.00000 val_loss: 0.16336, val_acc: 0.96667\n",
            "Epoch [8110/10000], loss: 0.09158 acc: 1.00000 val_loss: 0.16330, val_acc: 0.96667\n",
            "Epoch [8120/10000], loss: 0.09150 acc: 1.00000 val_loss: 0.16324, val_acc: 0.96667\n",
            "Epoch [8130/10000], loss: 0.09142 acc: 1.00000 val_loss: 0.16318, val_acc: 0.96667\n",
            "Epoch [8140/10000], loss: 0.09135 acc: 1.00000 val_loss: 0.16312, val_acc: 0.96667\n",
            "Epoch [8150/10000], loss: 0.09127 acc: 1.00000 val_loss: 0.16306, val_acc: 0.96667\n",
            "Epoch [8160/10000], loss: 0.09120 acc: 1.00000 val_loss: 0.16300, val_acc: 0.96667\n",
            "Epoch [8170/10000], loss: 0.09112 acc: 1.00000 val_loss: 0.16294, val_acc: 0.96667\n",
            "Epoch [8180/10000], loss: 0.09105 acc: 1.00000 val_loss: 0.16288, val_acc: 0.96667\n",
            "Epoch [8190/10000], loss: 0.09097 acc: 1.00000 val_loss: 0.16282, val_acc: 0.96667\n",
            "Epoch [8200/10000], loss: 0.09089 acc: 1.00000 val_loss: 0.16276, val_acc: 0.96667\n",
            "Epoch [8210/10000], loss: 0.09082 acc: 1.00000 val_loss: 0.16270, val_acc: 0.96667\n",
            "Epoch [8220/10000], loss: 0.09074 acc: 1.00000 val_loss: 0.16264, val_acc: 0.96667\n",
            "Epoch [8230/10000], loss: 0.09067 acc: 1.00000 val_loss: 0.16258, val_acc: 0.96667\n",
            "Epoch [8240/10000], loss: 0.09060 acc: 1.00000 val_loss: 0.16252, val_acc: 0.96667\n",
            "Epoch [8250/10000], loss: 0.09052 acc: 1.00000 val_loss: 0.16246, val_acc: 0.96667\n",
            "Epoch [8260/10000], loss: 0.09045 acc: 1.00000 val_loss: 0.16240, val_acc: 0.96667\n",
            "Epoch [8270/10000], loss: 0.09037 acc: 1.00000 val_loss: 0.16234, val_acc: 0.96667\n",
            "Epoch [8280/10000], loss: 0.09030 acc: 1.00000 val_loss: 0.16228, val_acc: 0.96667\n",
            "Epoch [8290/10000], loss: 0.09023 acc: 1.00000 val_loss: 0.16222, val_acc: 0.96667\n",
            "Epoch [8300/10000], loss: 0.09015 acc: 1.00000 val_loss: 0.16217, val_acc: 0.96667\n",
            "Epoch [8310/10000], loss: 0.09008 acc: 1.00000 val_loss: 0.16211, val_acc: 0.96667\n",
            "Epoch [8320/10000], loss: 0.09000 acc: 1.00000 val_loss: 0.16205, val_acc: 0.96667\n",
            "Epoch [8330/10000], loss: 0.08993 acc: 1.00000 val_loss: 0.16199, val_acc: 0.96667\n",
            "Epoch [8340/10000], loss: 0.08986 acc: 1.00000 val_loss: 0.16193, val_acc: 0.96667\n",
            "Epoch [8350/10000], loss: 0.08979 acc: 1.00000 val_loss: 0.16188, val_acc: 0.96667\n",
            "Epoch [8360/10000], loss: 0.08971 acc: 1.00000 val_loss: 0.16182, val_acc: 0.96667\n",
            "Epoch [8370/10000], loss: 0.08964 acc: 1.00000 val_loss: 0.16176, val_acc: 0.96667\n",
            "Epoch [8380/10000], loss: 0.08957 acc: 1.00000 val_loss: 0.16170, val_acc: 0.96667\n",
            "Epoch [8390/10000], loss: 0.08950 acc: 1.00000 val_loss: 0.16165, val_acc: 0.96667\n",
            "Epoch [8400/10000], loss: 0.08942 acc: 1.00000 val_loss: 0.16159, val_acc: 0.96667\n",
            "Epoch [8410/10000], loss: 0.08935 acc: 1.00000 val_loss: 0.16153, val_acc: 0.96667\n",
            "Epoch [8420/10000], loss: 0.08928 acc: 1.00000 val_loss: 0.16148, val_acc: 0.96667\n",
            "Epoch [8430/10000], loss: 0.08921 acc: 1.00000 val_loss: 0.16142, val_acc: 0.96667\n",
            "Epoch [8440/10000], loss: 0.08914 acc: 1.00000 val_loss: 0.16136, val_acc: 0.96667\n",
            "Epoch [8450/10000], loss: 0.08907 acc: 1.00000 val_loss: 0.16131, val_acc: 0.96667\n",
            "Epoch [8460/10000], loss: 0.08899 acc: 1.00000 val_loss: 0.16125, val_acc: 0.96667\n",
            "Epoch [8470/10000], loss: 0.08892 acc: 1.00000 val_loss: 0.16119, val_acc: 0.96667\n",
            "Epoch [8480/10000], loss: 0.08885 acc: 1.00000 val_loss: 0.16114, val_acc: 0.96667\n",
            "Epoch [8490/10000], loss: 0.08878 acc: 1.00000 val_loss: 0.16108, val_acc: 0.96667\n",
            "Epoch [8500/10000], loss: 0.08871 acc: 1.00000 val_loss: 0.16103, val_acc: 0.96667\n",
            "Epoch [8510/10000], loss: 0.08864 acc: 1.00000 val_loss: 0.16097, val_acc: 0.96667\n",
            "Epoch [8520/10000], loss: 0.08857 acc: 1.00000 val_loss: 0.16092, val_acc: 0.96667\n",
            "Epoch [8530/10000], loss: 0.08850 acc: 1.00000 val_loss: 0.16086, val_acc: 0.96667\n",
            "Epoch [8540/10000], loss: 0.08843 acc: 1.00000 val_loss: 0.16081, val_acc: 0.96667\n",
            "Epoch [8550/10000], loss: 0.08836 acc: 1.00000 val_loss: 0.16075, val_acc: 0.96667\n",
            "Epoch [8560/10000], loss: 0.08829 acc: 1.00000 val_loss: 0.16070, val_acc: 0.96667\n",
            "Epoch [8570/10000], loss: 0.08822 acc: 1.00000 val_loss: 0.16064, val_acc: 0.96667\n",
            "Epoch [8580/10000], loss: 0.08815 acc: 1.00000 val_loss: 0.16059, val_acc: 0.96667\n",
            "Epoch [8590/10000], loss: 0.08808 acc: 1.00000 val_loss: 0.16053, val_acc: 0.96667\n",
            "Epoch [8600/10000], loss: 0.08801 acc: 1.00000 val_loss: 0.16048, val_acc: 0.96667\n",
            "Epoch [8610/10000], loss: 0.08794 acc: 1.00000 val_loss: 0.16042, val_acc: 0.96667\n",
            "Epoch [8620/10000], loss: 0.08787 acc: 1.00000 val_loss: 0.16037, val_acc: 0.96667\n",
            "Epoch [8630/10000], loss: 0.08780 acc: 1.00000 val_loss: 0.16031, val_acc: 0.96667\n",
            "Epoch [8640/10000], loss: 0.08774 acc: 1.00000 val_loss: 0.16026, val_acc: 0.96667\n",
            "Epoch [8650/10000], loss: 0.08767 acc: 1.00000 val_loss: 0.16021, val_acc: 0.96667\n",
            "Epoch [8660/10000], loss: 0.08760 acc: 1.00000 val_loss: 0.16015, val_acc: 0.96667\n",
            "Epoch [8670/10000], loss: 0.08753 acc: 1.00000 val_loss: 0.16010, val_acc: 0.96667\n",
            "Epoch [8680/10000], loss: 0.08746 acc: 1.00000 val_loss: 0.16005, val_acc: 0.96667\n",
            "Epoch [8690/10000], loss: 0.08739 acc: 1.00000 val_loss: 0.15999, val_acc: 0.96667\n",
            "Epoch [8700/10000], loss: 0.08733 acc: 1.00000 val_loss: 0.15994, val_acc: 0.96667\n",
            "Epoch [8710/10000], loss: 0.08726 acc: 1.00000 val_loss: 0.15989, val_acc: 0.96667\n",
            "Epoch [8720/10000], loss: 0.08719 acc: 1.00000 val_loss: 0.15983, val_acc: 0.96667\n",
            "Epoch [8730/10000], loss: 0.08712 acc: 1.00000 val_loss: 0.15978, val_acc: 0.96667\n",
            "Epoch [8740/10000], loss: 0.08706 acc: 1.00000 val_loss: 0.15973, val_acc: 0.96667\n",
            "Epoch [8750/10000], loss: 0.08699 acc: 1.00000 val_loss: 0.15967, val_acc: 0.96667\n",
            "Epoch [8760/10000], loss: 0.08692 acc: 1.00000 val_loss: 0.15962, val_acc: 0.96667\n",
            "Epoch [8770/10000], loss: 0.08685 acc: 1.00000 val_loss: 0.15957, val_acc: 0.96667\n",
            "Epoch [8780/10000], loss: 0.08679 acc: 1.00000 val_loss: 0.15952, val_acc: 0.96667\n",
            "Epoch [8790/10000], loss: 0.08672 acc: 1.00000 val_loss: 0.15946, val_acc: 0.96667\n",
            "Epoch [8800/10000], loss: 0.08665 acc: 1.00000 val_loss: 0.15941, val_acc: 0.96667\n",
            "Epoch [8810/10000], loss: 0.08659 acc: 1.00000 val_loss: 0.15936, val_acc: 0.96667\n",
            "Epoch [8820/10000], loss: 0.08652 acc: 1.00000 val_loss: 0.15931, val_acc: 0.96667\n",
            "Epoch [8830/10000], loss: 0.08646 acc: 1.00000 val_loss: 0.15926, val_acc: 0.96667\n",
            "Epoch [8840/10000], loss: 0.08639 acc: 1.00000 val_loss: 0.15921, val_acc: 0.96667\n",
            "Epoch [8850/10000], loss: 0.08632 acc: 1.00000 val_loss: 0.15915, val_acc: 0.96667\n",
            "Epoch [8860/10000], loss: 0.08626 acc: 1.00000 val_loss: 0.15910, val_acc: 0.96667\n",
            "Epoch [8870/10000], loss: 0.08619 acc: 1.00000 val_loss: 0.15905, val_acc: 0.96667\n",
            "Epoch [8880/10000], loss: 0.08613 acc: 1.00000 val_loss: 0.15900, val_acc: 0.96667\n",
            "Epoch [8890/10000], loss: 0.08606 acc: 1.00000 val_loss: 0.15895, val_acc: 0.96667\n",
            "Epoch [8900/10000], loss: 0.08600 acc: 1.00000 val_loss: 0.15890, val_acc: 0.96667\n",
            "Epoch [8910/10000], loss: 0.08593 acc: 1.00000 val_loss: 0.15885, val_acc: 0.96667\n",
            "Epoch [8920/10000], loss: 0.08587 acc: 1.00000 val_loss: 0.15880, val_acc: 0.96667\n",
            "Epoch [8930/10000], loss: 0.08580 acc: 1.00000 val_loss: 0.15875, val_acc: 0.96667\n",
            "Epoch [8940/10000], loss: 0.08574 acc: 1.00000 val_loss: 0.15870, val_acc: 0.96667\n",
            "Epoch [8950/10000], loss: 0.08567 acc: 1.00000 val_loss: 0.15865, val_acc: 0.96667\n",
            "Epoch [8960/10000], loss: 0.08561 acc: 1.00000 val_loss: 0.15859, val_acc: 0.96667\n",
            "Epoch [8970/10000], loss: 0.08554 acc: 1.00000 val_loss: 0.15854, val_acc: 0.96667\n",
            "Epoch [8980/10000], loss: 0.08548 acc: 1.00000 val_loss: 0.15849, val_acc: 0.96667\n",
            "Epoch [8990/10000], loss: 0.08541 acc: 1.00000 val_loss: 0.15844, val_acc: 0.96667\n",
            "Epoch [9000/10000], loss: 0.08535 acc: 1.00000 val_loss: 0.15839, val_acc: 0.96667\n",
            "Epoch [9010/10000], loss: 0.08529 acc: 1.00000 val_loss: 0.15834, val_acc: 0.96667\n",
            "Epoch [9020/10000], loss: 0.08522 acc: 1.00000 val_loss: 0.15830, val_acc: 0.96667\n",
            "Epoch [9030/10000], loss: 0.08516 acc: 1.00000 val_loss: 0.15825, val_acc: 0.96667\n",
            "Epoch [9040/10000], loss: 0.08509 acc: 1.00000 val_loss: 0.15820, val_acc: 0.96667\n",
            "Epoch [9050/10000], loss: 0.08503 acc: 1.00000 val_loss: 0.15815, val_acc: 0.96667\n",
            "Epoch [9060/10000], loss: 0.08497 acc: 1.00000 val_loss: 0.15810, val_acc: 0.96667\n",
            "Epoch [9070/10000], loss: 0.08490 acc: 1.00000 val_loss: 0.15805, val_acc: 0.96667\n",
            "Epoch [9080/10000], loss: 0.08484 acc: 1.00000 val_loss: 0.15800, val_acc: 0.96667\n",
            "Epoch [9090/10000], loss: 0.08478 acc: 1.00000 val_loss: 0.15795, val_acc: 0.96667\n",
            "Epoch [9100/10000], loss: 0.08472 acc: 1.00000 val_loss: 0.15790, val_acc: 0.96667\n",
            "Epoch [9110/10000], loss: 0.08465 acc: 1.00000 val_loss: 0.15785, val_acc: 0.96667\n",
            "Epoch [9120/10000], loss: 0.08459 acc: 1.00000 val_loss: 0.15781, val_acc: 0.96667\n",
            "Epoch [9130/10000], loss: 0.08453 acc: 1.00000 val_loss: 0.15776, val_acc: 0.96667\n",
            "Epoch [9140/10000], loss: 0.08446 acc: 1.00000 val_loss: 0.15771, val_acc: 0.96667\n",
            "Epoch [9150/10000], loss: 0.08440 acc: 1.00000 val_loss: 0.15766, val_acc: 0.96667\n",
            "Epoch [9160/10000], loss: 0.08434 acc: 1.00000 val_loss: 0.15761, val_acc: 0.96667\n",
            "Epoch [9170/10000], loss: 0.08428 acc: 1.00000 val_loss: 0.15756, val_acc: 0.96667\n",
            "Epoch [9180/10000], loss: 0.08422 acc: 1.00000 val_loss: 0.15752, val_acc: 0.96667\n",
            "Epoch [9190/10000], loss: 0.08415 acc: 1.00000 val_loss: 0.15747, val_acc: 0.96667\n",
            "Epoch [9200/10000], loss: 0.08409 acc: 1.00000 val_loss: 0.15742, val_acc: 0.96667\n",
            "Epoch [9210/10000], loss: 0.08403 acc: 1.00000 val_loss: 0.15737, val_acc: 0.96667\n",
            "Epoch [9220/10000], loss: 0.08397 acc: 1.00000 val_loss: 0.15733, val_acc: 0.96667\n",
            "Epoch [9230/10000], loss: 0.08391 acc: 1.00000 val_loss: 0.15728, val_acc: 0.96667\n",
            "Epoch [9240/10000], loss: 0.08385 acc: 1.00000 val_loss: 0.15723, val_acc: 0.96667\n",
            "Epoch [9250/10000], loss: 0.08379 acc: 1.00000 val_loss: 0.15718, val_acc: 0.96667\n",
            "Epoch [9260/10000], loss: 0.08372 acc: 1.00000 val_loss: 0.15714, val_acc: 0.96667\n",
            "Epoch [9270/10000], loss: 0.08366 acc: 1.00000 val_loss: 0.15709, val_acc: 0.96667\n",
            "Epoch [9280/10000], loss: 0.08360 acc: 1.00000 val_loss: 0.15704, val_acc: 0.96667\n",
            "Epoch [9290/10000], loss: 0.08354 acc: 1.00000 val_loss: 0.15700, val_acc: 0.96667\n",
            "Epoch [9300/10000], loss: 0.08348 acc: 1.00000 val_loss: 0.15695, val_acc: 0.96667\n",
            "Epoch [9310/10000], loss: 0.08342 acc: 1.00000 val_loss: 0.15690, val_acc: 0.96667\n",
            "Epoch [9320/10000], loss: 0.08336 acc: 1.00000 val_loss: 0.15686, val_acc: 0.96667\n",
            "Epoch [9330/10000], loss: 0.08330 acc: 1.00000 val_loss: 0.15681, val_acc: 0.96667\n",
            "Epoch [9340/10000], loss: 0.08324 acc: 1.00000 val_loss: 0.15676, val_acc: 0.96667\n",
            "Epoch [9350/10000], loss: 0.08318 acc: 1.00000 val_loss: 0.15672, val_acc: 0.96667\n",
            "Epoch [9360/10000], loss: 0.08312 acc: 1.00000 val_loss: 0.15667, val_acc: 0.96667\n",
            "Epoch [9370/10000], loss: 0.08306 acc: 1.00000 val_loss: 0.15662, val_acc: 0.96667\n",
            "Epoch [9380/10000], loss: 0.08300 acc: 1.00000 val_loss: 0.15658, val_acc: 0.96667\n",
            "Epoch [9390/10000], loss: 0.08294 acc: 1.00000 val_loss: 0.15653, val_acc: 0.96667\n",
            "Epoch [9400/10000], loss: 0.08288 acc: 1.00000 val_loss: 0.15649, val_acc: 0.96667\n",
            "Epoch [9410/10000], loss: 0.08282 acc: 1.00000 val_loss: 0.15644, val_acc: 0.96667\n",
            "Epoch [9420/10000], loss: 0.08276 acc: 1.00000 val_loss: 0.15640, val_acc: 0.96667\n",
            "Epoch [9430/10000], loss: 0.08270 acc: 1.00000 val_loss: 0.15635, val_acc: 0.96667\n",
            "Epoch [9440/10000], loss: 0.08265 acc: 1.00000 val_loss: 0.15630, val_acc: 0.96667\n",
            "Epoch [9450/10000], loss: 0.08259 acc: 1.00000 val_loss: 0.15626, val_acc: 0.96667\n",
            "Epoch [9460/10000], loss: 0.08253 acc: 1.00000 val_loss: 0.15621, val_acc: 0.96667\n",
            "Epoch [9470/10000], loss: 0.08247 acc: 1.00000 val_loss: 0.15617, val_acc: 0.96667\n",
            "Epoch [9480/10000], loss: 0.08241 acc: 1.00000 val_loss: 0.15612, val_acc: 0.96667\n",
            "Epoch [9490/10000], loss: 0.08235 acc: 1.00000 val_loss: 0.15608, val_acc: 0.96667\n",
            "Epoch [9500/10000], loss: 0.08229 acc: 1.00000 val_loss: 0.15603, val_acc: 0.96667\n",
            "Epoch [9510/10000], loss: 0.08224 acc: 1.00000 val_loss: 0.15599, val_acc: 0.96667\n",
            "Epoch [9520/10000], loss: 0.08218 acc: 1.00000 val_loss: 0.15594, val_acc: 0.96667\n",
            "Epoch [9530/10000], loss: 0.08212 acc: 1.00000 val_loss: 0.15590, val_acc: 0.96667\n",
            "Epoch [9540/10000], loss: 0.08206 acc: 1.00000 val_loss: 0.15586, val_acc: 0.96667\n",
            "Epoch [9550/10000], loss: 0.08200 acc: 1.00000 val_loss: 0.15581, val_acc: 0.96667\n",
            "Epoch [9560/10000], loss: 0.08195 acc: 1.00000 val_loss: 0.15577, val_acc: 0.96667\n",
            "Epoch [9570/10000], loss: 0.08189 acc: 1.00000 val_loss: 0.15572, val_acc: 0.96667\n",
            "Epoch [9580/10000], loss: 0.08183 acc: 1.00000 val_loss: 0.15568, val_acc: 0.96667\n",
            "Epoch [9590/10000], loss: 0.08177 acc: 1.00000 val_loss: 0.15563, val_acc: 0.96667\n",
            "Epoch [9600/10000], loss: 0.08172 acc: 1.00000 val_loss: 0.15559, val_acc: 0.96667\n",
            "Epoch [9610/10000], loss: 0.08166 acc: 1.00000 val_loss: 0.15555, val_acc: 0.96667\n",
            "Epoch [9620/10000], loss: 0.08160 acc: 1.00000 val_loss: 0.15550, val_acc: 0.96667\n",
            "Epoch [9630/10000], loss: 0.08154 acc: 1.00000 val_loss: 0.15546, val_acc: 0.96667\n",
            "Epoch [9640/10000], loss: 0.08149 acc: 1.00000 val_loss: 0.15542, val_acc: 0.96667\n",
            "Epoch [9650/10000], loss: 0.08143 acc: 1.00000 val_loss: 0.15537, val_acc: 0.96667\n",
            "Epoch [9660/10000], loss: 0.08137 acc: 1.00000 val_loss: 0.15533, val_acc: 0.96667\n",
            "Epoch [9670/10000], loss: 0.08132 acc: 1.00000 val_loss: 0.15529, val_acc: 0.96667\n",
            "Epoch [9680/10000], loss: 0.08126 acc: 1.00000 val_loss: 0.15524, val_acc: 0.96667\n",
            "Epoch [9690/10000], loss: 0.08120 acc: 1.00000 val_loss: 0.15520, val_acc: 0.96667\n",
            "Epoch [9700/10000], loss: 0.08115 acc: 1.00000 val_loss: 0.15516, val_acc: 0.96667\n",
            "Epoch [9710/10000], loss: 0.08109 acc: 1.00000 val_loss: 0.15511, val_acc: 0.96667\n",
            "Epoch [9720/10000], loss: 0.08103 acc: 1.00000 val_loss: 0.15507, val_acc: 0.96667\n",
            "Epoch [9730/10000], loss: 0.08098 acc: 1.00000 val_loss: 0.15503, val_acc: 0.96667\n",
            "Epoch [9740/10000], loss: 0.08092 acc: 1.00000 val_loss: 0.15499, val_acc: 0.96667\n",
            "Epoch [9750/10000], loss: 0.08087 acc: 1.00000 val_loss: 0.15494, val_acc: 0.96667\n",
            "Epoch [9760/10000], loss: 0.08081 acc: 1.00000 val_loss: 0.15490, val_acc: 0.96667\n",
            "Epoch [9770/10000], loss: 0.08076 acc: 1.00000 val_loss: 0.15486, val_acc: 0.96667\n",
            "Epoch [9780/10000], loss: 0.08070 acc: 1.00000 val_loss: 0.15482, val_acc: 0.96667\n",
            "Epoch [9790/10000], loss: 0.08064 acc: 1.00000 val_loss: 0.15477, val_acc: 0.96667\n",
            "Epoch [9800/10000], loss: 0.08059 acc: 1.00000 val_loss: 0.15473, val_acc: 0.96667\n",
            "Epoch [9810/10000], loss: 0.08053 acc: 1.00000 val_loss: 0.15469, val_acc: 0.96667\n",
            "Epoch [9820/10000], loss: 0.08048 acc: 1.00000 val_loss: 0.15465, val_acc: 0.96667\n",
            "Epoch [9830/10000], loss: 0.08042 acc: 1.00000 val_loss: 0.15461, val_acc: 0.96667\n",
            "Epoch [9840/10000], loss: 0.08037 acc: 1.00000 val_loss: 0.15456, val_acc: 0.96667\n",
            "Epoch [9850/10000], loss: 0.08031 acc: 1.00000 val_loss: 0.15452, val_acc: 0.96667\n",
            "Epoch [9860/10000], loss: 0.08026 acc: 1.00000 val_loss: 0.15448, val_acc: 0.96667\n",
            "Epoch [9870/10000], loss: 0.08020 acc: 1.00000 val_loss: 0.15444, val_acc: 0.96667\n",
            "Epoch [9880/10000], loss: 0.08015 acc: 1.00000 val_loss: 0.15440, val_acc: 0.96667\n",
            "Epoch [9890/10000], loss: 0.08009 acc: 1.00000 val_loss: 0.15436, val_acc: 0.96667\n",
            "Epoch [9900/10000], loss: 0.08004 acc: 1.00000 val_loss: 0.15432, val_acc: 0.96667\n",
            "Epoch [9910/10000], loss: 0.07999 acc: 1.00000 val_loss: 0.15427, val_acc: 0.96667\n",
            "Epoch [9920/10000], loss: 0.07993 acc: 1.00000 val_loss: 0.15423, val_acc: 0.96667\n",
            "Epoch [9930/10000], loss: 0.07988 acc: 1.00000 val_loss: 0.15419, val_acc: 0.96667\n",
            "Epoch [9940/10000], loss: 0.07982 acc: 1.00000 val_loss: 0.15415, val_acc: 0.96667\n",
            "Epoch [9950/10000], loss: 0.07977 acc: 1.00000 val_loss: 0.15411, val_acc: 0.96667\n",
            "Epoch [9960/10000], loss: 0.07972 acc: 1.00000 val_loss: 0.15407, val_acc: 0.96667\n",
            "Epoch [9970/10000], loss: 0.07966 acc: 1.00000 val_loss: 0.15403, val_acc: 0.96667\n",
            "Epoch [9980/10000], loss: 0.07961 acc: 1.00000 val_loss: 0.15399, val_acc: 0.96667\n",
            "Epoch [9990/10000], loss: 0.07955 acc: 1.00000 val_loss: 0.15395, val_acc: 0.96667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q　結果確認"
      ],
      "metadata": {
        "id": "lzg6wVTRNBac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#損失と精度の確認\n",
        "\n",
        "print(f'初期状態: 損失: {history[0,3]:.5f} 精度: {history[0,4]:.5f}' )\n",
        "print(f'最終状態: 損失: {history[-1,3]:.5f} 精度: {history[-1,4]:.5f}' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_kcUQxKNLjt",
        "outputId": "dc1dc73b-3e63-4d49-c8de-b7b21e46ff44"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "初期状態: 損失: 4.49384 精度: 0.50000\n",
            "最終状態: 損失: 0.15395 精度: 0.96667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習曲線の表示 (損失)\n",
        "\n",
        "plt.plot(history[:,0], history[:,1], 'b', label='訓練')\n",
        "plt.plot(history[:,0], history[:,3], 'k', label='検証')\n",
        "plt.xlabel('繰り返し回数')\n",
        "plt.ylabel('損失')\n",
        "plt.title('学習曲線(損失)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "QDcwVsVON4pC",
        "outputId": "25564549-04c7-4482-873b-59b2a5ac4509"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習曲線の表示 (精度)\n",
        "\n",
        "plt.plot(history[:,0], history[:,2], 'b', label='訓練')\n",
        "plt.plot(history[:,0], history[:,4], 'k', label='検証')\n",
        "plt.xlabel('繰り返し回数')\n",
        "plt.ylabel('精度')\n",
        "plt.title('学習曲線(精度)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "i3e9W_qHN7hA",
        "outputId": "9e4eb04e-5c33-4f90-f78a-09036415c883"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回作ったロジスティック回帰モデルでは、2つの分類結果の境界になる直線が存在し、その直線を決定境界と呼びます。"
      ],
      "metadata": {
        "id": "xsxhilHoOa7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 検証データを散布図用に準備\n",
        "\n",
        "x_t0 = x_test[y_test==0]\n",
        "x_t1 = x_test[y_test==1]\n",
        "\n",
        "# パラメータの取得\n",
        "\n",
        "bias = net.l1.bias.data.numpy()\n",
        "weight = net.l1.weight.data.numpy()\n",
        "print(f'BIAS = {bias}, WEIGHT = {weight}')\n",
        "\n",
        "# 決定境界描画用 x1の値から x2の値を計算する\n",
        "def decision(x):\n",
        "    return(-(bias + weight[0,0] * x)/ weight[0,1])\n",
        "\n",
        "# 散布図のx1の最小値と最大値\n",
        "xl = np.array([x_test[:,0].min(), x_test[:,0].max()])\n",
        "yl = decision(xl)\n",
        "\n",
        "# 結果確認\n",
        "print(f'xl = {xl}  yl = {yl}')\n",
        "\n",
        "# 散布図表示\n",
        "plt.scatter(x_t0[:,0], x_t0[:,1], marker='x', \n",
        "        c='b', s=50, label='class 0')\n",
        "plt.scatter(x_t1[:,0], x_t1[:,1], marker='o', \n",
        "        c='k', s=50, label='class 1')\n",
        "\n",
        "# 決定境界直線\n",
        "plt.plot(xl, yl, c='b')\n",
        "plt.xlabel('sepal_length')\n",
        "plt.ylabel('sepal_width')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "TKuDkHN6O3i3",
        "outputId": "8e9eca69-1dff-4ac2-a8eb-6f3033d696fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BIAS = [0.3386128], WEIGHT = [[ 2.9700322 -5.300015 ]]\n",
            "xl = [4.4 7. ]  yl = [2.52956918 3.986562  ]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}